{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ],
   "id": "b76a1e3e9b344be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "98dd936c0a392494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:05:16.799009Z",
     "start_time": "2025-11-13T06:05:16.177134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from stop_words import get_stop_words\n",
    "from ngramkit.ngram_filter.lemmatizer import SpacyLemmatizer\n",
    "from ngramkit.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramkit.ngram_filter.config import PipelineConfig as FilterPipelineConfig\n",
    "from ngramkit.ngram_filter.config import FilterConfig\n",
    "from ngramkit.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramkit.ngram_pivot.config import PipelineConfig as PivotPipelineConfig\n",
    "from ngramkit.ngram_pivot.pipeline import run_pivot_pipeline\n",
    "from ngramkit.utilities.peek import db_head, db_peek, db_peek_prefix\n",
    "from ngramkit.utilities.notebook_logging import setup_notebook_logging"
   ],
   "id": "f261bd4a6317873c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": "### Configure"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:05:16.954927Z",
     "start_time": "2025-11-13T06:05:16.802448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "release = '20200217'\n",
    "language = 'rus'\n",
    "size = 5\n",
    "num_files = 633"
   ],
   "id": "c898aed37fe8fa78",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "config_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:05:16.977437Z",
     "start_time": "2025-11-13T06:05:16.958252Z"
    }
   },
   "source": [
    "base_path = Path(f\"/scratch/edk202/NLP_corpora/Google_Books/{release}/{language}/{size}gram_files\")\n",
    "raw_db = base_path / f\"{size}grams.db\"\n",
    "filtered_db = base_path / f\"{size}grams_processed.db\"\n",
    "pivoted_db = base_path / f\"{size}grams_pivoted.db\"\n",
    "filter_tmp_dir = base_path / \"processing_tmp\"\n",
    "pivot_tmp_dir = base_path / \"pivot_tmp\"\n",
    "whitelist_path = f\"/scratch/edk202/NLP_corpora/Google_Books/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": "## **Step 1: Download and Ingest**"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T05:16:24.358947Z",
     "start_time": "2025-11-13T05:09:23.518111Z"
    }
   },
   "source": [
    "setup_notebook_logging(\n",
    "    workflow_name=\"multigrams_acquire\",\n",
    "    console=False\n",
    ")\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=\"/scratch/edk202/NLP_corpora/Google_Books/\",\n",
    "    file_range=(0, num_files-1),\n",
    "    workers=99,\n",
    
    "    ngram_type=\"tagged\",\n",
    "    overwrite_db=True,\n",
    "    write_batch_size=1_000_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-13 00:09:23\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/rus/5-\n",
      "DB path:              /scratch/edk202/NLP_corpora/Google_Books/20200217/rus/5gram_files/5grams.db\n",
      "File range:           0 to 632\n",
      "Total files:          633\n",
      "Files to get:         633\n",
      "Skipping:             0\n",
      "Download workers:     99\n",
      "Batch size:           1,000,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         True\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|███████████████████████████████████████████████████████| 633/633 [05:38<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Ingestion Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         36.11 GB\n",
      "Compaction completed in 0:01:20\n",
      "Size before:             36.11 GB\n",
      "Size after:              37.76 GB\n",
      "Space saved:             -1.65 GB (-4.6%)\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       633\n",
      "Failed files:                0\n",
      "Total entries written:       50,094,861\n",
      "Write batches flushed:       234\n",
      "Uncompressed data processed: 1.36 TB\n",
      "Processing throughput:       3383.32 MB/sec\n",
      "\n",
      "End Time: 2025-11-13 00:16:24.343211\n",
      "Total Runtime: 0:07:00.795373\n",
      "Time per file: 0:00:00.664764\n",
      "Files per hour: 5415.5\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing `FilterConfig` and `FilterPipelineConfig` objects to the `build_processed_db` function. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did in the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:03:12.062269Z",
     "start_time": "2025-11-13T05:46:54.560991Z"
    }
   },
   "source": [
    "setup_notebook_logging(\n",
    "    workflow_name=\"multigrams_filter\",\n",
    "    data_path=str(base_path),\n",
    "    console=False\n",
    ")\n",
    "\n",
    "stop_set = set(get_stop_words(\"russian\"))\n",
    "lemmatizer = SpacyLemmatizer(language=\"ru\")\n",
    "\n",
    "filter_config = FilterConfig(\n",
    "    stop_set=stop_set,\n",
    "    lemma_gen=lemmatizer,\n",
    "    whitelist_path=whitelist_path\n",
    ")\n",
    "\n",
    "pipeline_config = FilterPipelineConfig(\n",
    "    src_db=raw_db,\n",
    "    dst_db=filtered_db,\n",
    "    tmp_dir=filter_tmp_dir,\n",
    "    num_workers=50,\n",
    "    use_smart_partitioning=True,\n",
    "    num_initial_work_units=500,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    samples_per_worker=500_000,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    flush_interval_s=5.0,\n",
    "    mode=\"restart\",\n",
    "    progress_every_s=15.0,\n",
    "    ingest_num_readers=10,\n",
    "    ingest_batch_items=1_000_000,\n",
    "    ingest_queue_size=3,\n",
    "    compact_after_ingest=True\n",
    ")\n",
    "\n",
    "build_processed_db(pipeline_config, filter_config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-13 00:46:56\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/rus/5gram_files/5grams.db\n",
      "Target DB:            ...dk202/NLP_corpora/Google_Books/20200217/rus/5gram_files/5grams_processed.db\n",
      "Temp directory:       ...tch/edk202/NLP_corpora/Google_Books/20200217/rus/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              50\n",
      "Initial work units:   500\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       10\n",
      "Queue size:           3\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...ora/Google_Books/20200217/rus/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 6,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Sampling database to create 500 density-based work units...\n",
      "Created 500 balanced work units based on data density\n",
      "Cached partition results for future use\n",
      "\n",
      "Phase 2: Processing 500 work units with 50 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    50.75K        88.7%          4/50        496·4·0        3.3k/s         15s      \n",
      "   236.08K        87.6%          8/50        491·8·1        7.8k/s         30s      \n",
      "   641.83K        88.6%         12/50        483·12·5      14.2k/s         45s      \n",
      "    1.20M         86.9%         16/50       472·16·12      20.0k/s        1m00s     \n",
      "    1.94M         84.9%         20/50       459·20·21      25.8k/s        1m15s     \n",
      "    2.91M         85.5%         24/50       448·24·28      32.2k/s        1m30s     \n",
      "    4.01M         85.4%         28/50       428·28·44      38.1k/s        1m45s     \n",
      "    5.30M         84.5%         32/50       410·32·58      44.0k/s        2m00s     \n",
      "    6.75M         83.6%         37/50       387·37·76      49.9k/s        2m15s     \n",
      "    8.36M         83.9%         41/50       356·41·103     55.6k/s        2m30s     \n",
      "    10.10M        84.4%         46/50       326·46·128     61.1k/s        2m45s     \n",
      "    12.02M        84.2%         50/50       286·50·164     66.6k/s        3m00s     \n",
      "    14.00M        84.3%         50/50       258·50·192     71.7k/s        3m15s     \n",
      "    16.03M        84.6%         50/50       231·50·219     76.2k/s        3m30s     \n",
      "    18.08M        84.9%         50/50       211·50·239     80.3k/s        3m45s     \n",
      "    20.12M        85.0%         50/50       183·50·267     83.7k/s        4m00s     \n",
      "    22.14M        85.2%         50/50       168·50·282     86.7k/s        4m15s     \n",
      "    24.14M        85.5%         50/50       159·50·291     89.3k/s        4m30s     \n",
      "    26.15M        85.5%         50/50       146·50·304     91.7k/s        4m45s     \n",
      "    28.22M        85.6%         50/50       136·50·314     94.0k/s        5m00s     \n",
      "    30.23M        85.6%         50/50       123·50·327     95.9k/s        5m15s     \n",
      "    32.32M        85.5%         50/50       99·50·351      97.8k/s        5m30s     \n",
      "    34.34M        85.3%         50/50       80·50·370      99.5k/s        5m45s     \n",
      "    36.37M        85.1%         50/50       52·50·398      100.9k/s       6m00s     \n",
      "    38.27M        85.0%         50/50       19·50·431      102.0k/s       6m15s     \n",
      "    40.14M        85.0%         44/50        0·44·456      102.8k/s       6m30s     \n",
      "    41.81M        85.2%         38/50        0·38·462      103.2k/s       6m45s     \n",
      "    43.23M        85.3%         32/50        0·32·468      102.9k/s       7m00s     \n",
      "    44.45M        85.5%         30/50        0·30·470      102.1k/s       7m15s     \n",
      "    45.57M        85.5%         28/50        0·28·472      101.2k/s       7m30s     \n",
      "    46.70M        85.7%         26/50        0·26·474      100.4k/s       7m45s     \n",
      "    47.71M        85.8%         20/50        0·20·480      99.3k/s        8m00s     \n",
      "    48.43M        85.8%         17/50        0·17·483      97.8k/s        8m15s     \n",
      "    49.03M        85.9%         14/50        0·14·486      96.1k/s        8m30s     \n",
      "    49.43M        85.9%          8/50        0·8·492       94.1k/s        8m45s     \n",
      "    49.64M        85.9%          5/50        0·5·495       91.9k/s        9m00s     \n",
      "    49.79M        85.9%          3/50        0·3·497       89.7k/s        9m15s     \n",
      "    49.87M        85.9%          2/50        0·2·498       87.4k/s        9m30s     \n",
      "    49.93M        85.9%          1/50        0·1·499       85.3k/s        9m45s     \n",
      "    49.94M        85.9%          1/50        0·1·499       83.2k/s        10m00s    \n",
      "    49.96M        85.9%          1/50        0·1·499       81.2k/s        10m15s    \n",
      "    49.97M        85.9%          1/50        0·1·499       79.3k/s        10m30s    \n",
      "    49.99M        85.9%          1/50        0·1·499       77.5k/s        10m45s    \n",
      "    50.03M        85.9%          1/50        0·1·499       75.8k/s        11m00s    \n",
      "    50.08M        85.9%          1/50        0·1·499       74.2k/s        11m15s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    50.09M        85.9%          0/50        0·0·500       73.9k/s        11m18s    \n",
      "\n",
      "Phase 3: Ingesting 500 shards with 10 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 500/500 [02:17<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 500 shards, 18,567,640 items in 137.7s (134,881 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         11.61 GB\n",
      "Compaction completed in 0:00:27\n",
      "Size before:             11.61 GB\n",
      "Size after:              10.39 GB\n",
      "Space saved:             1.22 GB (10.5%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 14,296,958 (estimated)                                                                    │\n",
      "│ Size: 10.39 GB                                                                                   │\n",
      "│ Database: ...atch/edk202/NLP_corpora/Google_Books/20200217/rus/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": "## **Step 3: Pivot to Yearly Indices**"
  },
  {
   "cell_type": "code",
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:08:50.184875Z",
     "start_time": "2025-11-13T06:05:23.807365Z"
    }
   },
   "source": [
    "setup_notebook_logging(\n",
    "    workflow_name=\"multigrams_pivot\",\n",
    "    data_path=str(base_path),\n",
    "    console=False\n",
    ")\n",
    "\n",
    "pivot_config = PivotPipelineConfig(\n",
    "    src_db=filtered_db,\n",
    "    dst_db=pivoted_db,\n",
    "    tmp_dir=pivot_tmp_dir,\n",
    "    num_workers=24,\n",
    "    num_initial_work_units=500,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    samples_per_worker=500_000,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    flush_interval_s=15.0,\n",
    "    progress_every_s=10.0,\n",
    "    mode=\"restart\",\n",
    "    num_ingest_readers=1,\n",
    "    ingest_buffer_shards=1,\n",
    "    use_smart_partitioning=True,\n",
    "    ingest_mode=\"direct_sst\"\n",
    ")\n",
    "\n",
    "run_pivot_pipeline(pivot_config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-13 01:05:23\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...dk202/NLP_corpora/Google_Books/20200217/rus/5gram_files/5grams_processed.db\n",
      "Target DB:            .../edk202/NLP_corpora/Google_Books/20200217/rus/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       /scratch/edk202/NLP_corpora/Google_Books/20200217/rus/5gram_files/pivot_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              24\n",
      "Initial work units:   500\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       1\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       15.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Sampling database to create 500 density-based work units...\n",
      "Created 500 balanced work units based on data density\n",
      "Cached partition results for future use\n",
      "\n",
      "Phase 2: Processing 500 work units with 24 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     2.67M           34.9x         431·19·50        261.6k/s          10s       \n",
      "     7.52M           34.7x         344·17·139       372.3k/s          20s       \n",
      "     11.75M          34.7x         250·23·227       388.7k/s          30s       \n",
      "     17.38M          34.9x         138·22·340       432.3k/s          40s       \n",
      "     23.00M          34.8x         28·24·448        458.2k/s          50s       \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "     25.55M          34.8x          0·0·500         460.6k/s          55s       \n",
      "\n",
      "Phase 3: Ingesting 500 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 500/500 [00:26<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         9.07 GB\n",
      "Compaction completed in 0:01:36\n",
      "Size before:             9.07 GB\n",
      "Size after:              21.17 GB\n",
      "Space saved:             -12.10 GB (-133.4%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 444,807,860 (estimated)                                                                   │\n",
      "│ Size: 21.17 GB                                                                                   │\n",
      "│ Database: /scratch/edk202/NLP_corpora/Google_Books/20200217/rus/5gram_files/5grams_pivoted.db    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:08:59.895315Z",
     "start_time": "2025-11-13T06:08:59.647220Z"
    }
   },
   "source": "db_head(str(pivoted_db), n=10)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1486] <UNK> <UNK> <UNK> <UNK> Италия\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 2] Key:   [1486] <UNK> <UNK> <UNK> <UNK> Надо\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1486] <UNK> <UNK> <UNK> <UNK> При\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1486] <UNK> <UNK> <UNK> <UNK> автор\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1486] <UNK> <UNK> <UNK> <UNK> акт\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 6] Key:   [1486] <UNK> <UNK> <UNK> <UNK> ангел\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 7] Key:   [1486] <UNK> <UNK> <UNK> <UNK> апостол\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 8] Key:   [1486] <UNK> <UNK> <UNK> <UNK> благодаря\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 9] Key:   [1486] <UNK> <UNK> <UNK> <UNK> бог\n",
      "     Value: 6 occurrences in 6 documents\n",
      "\n",
      "[10] Key:   [1486] <UNK> <UNK> <UNK> <UNK> божественный\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_peek",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:11:32.353165Z",
     "start_time": "2025-11-13T06:11:32.221717Z"
    }
   },
   "source": "db_peek(str(pivoted_db), start_key=\"[1964] <UNK> ядерный бомба <UNK> <UNK> <UNK>\", n=5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007ac3c554e4b3e20d18fd0b4d0b5d180d0bdd18bd0b920d0b1d0bed0bcd0b1d0b0203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1964] <UNK> ядерный бомба <UNK> борт\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n",
      "[ 2] Key:   [1964] <UNK> ядерный веко <UNK> <UNK>\n",
      "     Value: 6 occurrences in 6 documents\n",
      "\n",
      "[ 3] Key:   [1964] <UNK> ядерный взрыв <UNK> <UNK>\n",
      "     Value: 8 occurrences in 7 documents\n",
      "\n",
      "[ 4] Key:   [1964] <UNK> ядерный взрыв <UNK> атмосфера\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 5] Key:   [1964] <UNK> ядерный взрыв <UNK> включая\n",
      "     Value: 10 occurrences in 10 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "inspect_prefix_header",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T06:14:52.288176Z",
     "start_time": "2025-11-13T06:14:52.156785Z"
    }
   },
   "source": "db_peek_prefix(str(pivoted_db), prefix=\"[2019] <UNK> <UNK> нищий <UNK> <UNK>\", n=5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007e33c554e4b3e203c554e4b3e20d0bdd0b8d189d0b8d0b9203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2019] <UNK> <UNK> нищий <UNK> <UNK>\n",
      "     Value: 1,289 occurrences in 1,175 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24a850b70859d225"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
