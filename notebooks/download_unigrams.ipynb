{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Setup**\n",
    "## Recompile Cython Extensions"
   ],
   "id": "b784e1a1cda09447"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:08:02.169215Z",
     "start_time": "2025-10-04T18:07:47.523697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%cd /scratch/edk202/ngram-prep\n",
    "\n",
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "\n",
    "%pip install -e . --no-build-isolation -q"
   ],
   "id": "1ad267332e4b1fcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/edk202/ngram-prep\n",
      "env: LC_ALL=C.UTF-8\n",
      "env: LANG=C.UTF-8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "a8e3ef360e1fe339"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:08:02.934050Z",
     "start_time": "2025-10-04T18:08:02.173300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Auto-reload edited scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# NLTK resources\n",
    "from nltk.corpus import stopwords; stopwords = set(stopwords.words(\"english\"))\n",
    "from nltk.stem import WordNetLemmatizer; lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Ngram acquisition functions\n",
    "from ngram_acquire.pipeline.orchestrate import download_and_ingest_to_rocksdb\n",
    "from ngram_acquire.pipeline.logger import setup_logger\n",
    "\n",
    "# Ngram processing functions\n",
    "from pathlib import Path\n",
    "from ngram_filter.config import PipelineConfig, FilterConfig\n",
    "from ngram_filter.pipeline.orchestrator import build_processed_db"
   ],
   "id": "1998c9fafc32aa8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up logging to file",
   "id": "abd0247891250279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:08:03.027095Z",
     "start_time": "2025-10-04T18:08:02.938715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "setup_logger(\n",
    "    db_path=\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams.db\",\n",
    "    console=False,\n",
    "    rotate=True,\n",
    "    max_bytes=100_000_000,\n",
    "    backup_count=5,\n",
    "    force=True\n",
    ")"
   ],
   "id": "34a1e1724ae74cc1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams.db/ngram_download_20251004_140803.log')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Download Unigrams and Ingest to RocksDB**",
   "id": "879927f391b65d8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:16:18.004362Z",
     "start_time": "2025-10-04T18:08:03.030634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=1,\n",
    "    repo_release_id=\"20200217\",\n",
    "    repo_corpus_id=\"eng\",\n",
    "    db_path_stub=\"/vast/edk202/NLP_corpora/Google_Books/\",\n",
    "    file_range=(0, 23),\n",
    "    random_seed=21,\n",
    "    workers=30,\n",
    "    use_threads=False,\n",
    "    ngram_type=\"tagged\",\n",
    "    overwrite_db=True,\n",
    "    overwrite_checkpoint=True,\n",
    "    write_batch_size=100_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    post_compact=False,\n",
    ")"
   ],
   "id": "23449b75707ae1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-10-04 14:08:03\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng/1-\n",
      "DB path:              /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams.db\n",
      "File range:           0 to 23\n",
      "Total files:          24\n",
      "Files to get:         24\n",
      "Skipping:             0\n",
      "Download workers:     30\n",
      "Batch size:           100,000\n",
      "Ngram size:           1\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         True\n",
      "Overwrite checkpoint: False\n",
      "DB Profile:           write:packed24\n",
      "Compact:              False\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|█████████████████████████████████████████████████████████| 24/24 [07:35<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       24\n",
      "Failed files:                0\n",
      "Total entries written:       41,783,218\n",
      "Write batches flushed:       24\n",
      "Uncompressed data processed: 43.28 GB\n",
      "Processing throughput:       89.54 MB/sec\n",
      "\n",
      "End Time: 2025-10-04 14:16:17.999999\n",
      "Total Runtime: 0:08:14.945866\n",
      "Time per file: 0:00:20.622744\n",
      "Files per hour: 174.6\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Run Processing Pipeline**",
   "id": "97d09d26c6f2043f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:38:04.997318Z",
     "start_time": "2025-10-04T18:16:20.377226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_db = Path(\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams.db\")\n",
    "dst_db = src_db.parent / \"1grams_processed.db\"\n",
    "tmp_dir = src_db.parent / \"processing_tmp\"\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    src_db=src_db,\n",
    "    dst_db=dst_db,\n",
    "    tmp_dir=tmp_dir,\n",
    "    readers=32,\n",
    "    ingestors=32,\n",
    "    work_units_per_reader=1,\n",
    "    partitioning_sample_rate=0.01,\n",
    "    prefix_length=4,\n",
    "    mode=\"restart\",\n",
    "    force_cache_use=False,\n",
    "    enable_ingest=True,\n",
    "    delete_after_ingest=True,\n",
    "    post_compact=True,\n",
    "    overwrite_checkpoint=True,\n",
    "    progress_every_s=60.0,\n",
    "    output_whitelist_path=dst_db / \"whitelist.txt\",\n",
    "    output_whitelist_top_n=40_000\n",
    ")\n",
    "\n",
    "filter_config = FilterConfig(\n",
    "    stop_set=stopwords,\n",
    "    lemma_gen=lemmatizer,\n",
    ")\n",
    "\n",
    "build_processed_db(pipeline_config, filter_config)"
   ],
   "id": "57db436f73cba01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "Configuration:\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\u001B[4mPipeline\u001B[0m\n",
      "Run mode: restart\n",
      "Ingest after filtering: True\n",
      "Compact after ingesting: True\n",
      "  \n",
      "\u001B[4mWorkers\u001B[0m\n",
      "Num Workers: 32\n",
      "Work units: 32\n",
      "Profiles: read=read:packed24, write=write:packed24\n",
      "Buffer: 100,000 items, 128MB\n",
      "  \n",
      "\u001B[4mFiles\u001B[0m\n",
      "Source: /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams.db\n",
      "Destination: /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams_processed.db\n",
      "Input whitelist: None\n",
      "Output whitelist: ...ks/20200217/eng/1gram_files/1grams_processed.db/whitelist.txt (top 40,000 keys)\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - resampling and creating new work units\n",
      "Sampling database at 0.01000 rate (prefix_length=4)...\n",
      "Targeting 417,832 samples using reservoir sampling\n",
      "\n",
      "  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "  RESERVOIR SAMPLING CONFIGURATION\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  Target sample size:     417,832 items\n",
      "  Database size:          41,783,242 items\n",
      "  Progress reporting:     Every 10% complete\n",
      "  Database limit:         No limit (full traversal)\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  Progress: 10.0% (4,178,325 items)\n",
      "  Progress: 20.0% (8,356,649 items)\n",
      "  Progress: 30.0% (12,534,973 items)\n",
      "  Progress: 40.0% (16,713,297 items)\n",
      "  Progress: 50.0% (20,891,621 items)\n",
      "  Progress: 60.0% (25,069,946 items)\n",
      "  Progress: 70.0% (29,248,270 items)\n",
      "  Progress: 80.0% (33,426,594 items)\n",
      "  Progress: 90.0% (37,604,918 items)\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  RESERVOIR SAMPLING RESULTS\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  Items processed:        41,780,822\n",
      "  Metadata entries:       2,420\n",
      "  Final sample size:      417,832\n",
      "  Execution time:         353.0121 seconds\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  PERFORMANCE METRICS\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  Processing rate:        118,355 items/second\n",
      "  Time per item:          8.45 microseconds\n",
      "  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "Sampling complete: 417,832 samples collected, 154410 unique prefixes\n",
      "\n",
      "Creating 32 work units targeting 1,305,725 records each:\n",
      "  Unit 0: start → 312c3731 (~1,306,100 records)\n",
      "  Unit 1: 312c3731 → 31353164 (~1,305,800 records)\n",
      "  Unit 2: 31353164 → 322f332e (~1,305,800 records)\n",
      "  Unit 3: 322f332e → 332c313a (~1,306,100 records)\n",
      "  Unit 4: 332c313a → 3430392d (~1,305,800 records)\n",
      "  Unit 5: 3430392d → 35392c32 (~1,306,200 records)\n",
      "  Unit 6: 35392c32 → 3738382d (~1,306,700 records)\n",
      "  Unit 7: 3738382d → 41313250 (~1,305,800 records)\n",
      "  Unit 8: 41313250 → 42616363 (~1,305,800 records)\n",
      "  Unit 9: 42616363 → 43656c74 (~1,306,000 records)\n",
      "  Unit 10: 43656c74 → 446f7071 (~1,306,000 records)\n",
      "  Unit 11: 446f7071 → 47434167 (~1,305,800 records)\n",
      "  Unit 12: 47434167 → 4930306f (~1,305,800 records)\n",
      "  Unit 13: 4930306f → 4b6f6462 (~1,306,000 records)\n",
      "  Unit 14: 4b6f6462 → 4d617973 (~1,305,900 records)\n",
      "  Unit 15: 4d617973 → 4f6a696a (~1,305,800 records)\n",
      "  Unit 16: 4f6a696a → 51617573 (~1,305,800 records)\n",
      "  Unit 17: 51617573 → 53636878 (~1,308,200 records)\n",
      "  Unit 18: 53636878 → 54657375 (~1,305,900 records)\n",
      "  Unit 19: 54657375 → 57686975 (~1,308,100 records)\n",
      "  Unit 20: 57686975 → 61707973 (~1,305,800 records)\n",
      "  Unit 21: 61707973 → 6369656f (~1,306,200 records)\n",
      "  Unit 22: 6369656f → 64697369 (~1,307,000 records)\n",
      "  Unit 23: 64697369 → 666c7570 (~1,305,900 records)\n",
      "  Unit 24: 666c7570 → 696b6d60 (~1,305,800 records)\n",
      "  Unit 25: 696b6d60 → 6c696974 (~1,305,800 records)\n",
      "  Unit 26: 6c696974 → 6e69746a (~1,306,400 records)\n",
      "  Unit 27: 6e69746a → 7069706c (~1,305,800 records)\n",
      "  Unit 28: 7069706c → 72696366 (~1,305,800 records)\n",
      "  Unit 29: 72696366 → 73756364 (~1,309,300 records)\n",
      "  Unit 30: 73756364 → 75706b6f (~1,305,800 records)\n",
      "  Unit 31: 75706b6f → end (~300 records)\n",
      "\n",
      "Created 32 balanced work units\n",
      "Ensured gapless coverage across all work units\n",
      "\n",
      "Work unit balance analysis:\n",
      "  Average: 1,305,725 records per unit\n",
      "  Range: 1,290,200 to 1,309,300\n",
      "  Ratio: 1.0x difference\n",
      "\n",
      "Sorted 32 work units by size (largest first) for optimal parallelization\n",
      "Saved work units to cache: ...orpora/Google_Books/20200217/eng/1gram_files/1grams.db.work_units.json\n",
      "\n",
      "Phase 2: Processing 32 work units with 32 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    recs scanned     recs written     items kept       throughput       elapsed\n",
      "   ───────────────────────────────────────────────────────────────────────────────\n",
      "    34,589,547       18,466,740       56%              573.9k/s         1m00s         \n",
      "   ──────────────────────────────────── final ────────────────────────────────────\n",
      "    41,783,242       25,127,726       60%              353.3k/s         1m58s         \n",
      "\n",
      "Phase 3: Merging worker outputs into final database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ingesting shards with batch size 128MB / 200,000 items\n",
      "Deleting shards after successful ingestion\n",
      "\n",
      "Folding 32 shard(s) with 32 parallel readers...\n",
      "  unit_0006.db: 0 items (0.0 MB) [deleted]\n",
      "  unit_0001.db: 0 items (0.0 MB) [deleted]\n",
      "  unit_0002.db: 0 items (0.0 MB) [deleted]\n",
      "  unit_0003.db: 0 items (0.0 MB) [deleted]\n",
      "  unit_0004.db: 0 items (0.0 MB) [deleted]\n",
      "  unit_0007.db: 0 items (0.0 MB) [deleted]\n",
      "  unit_0000.db: 0 items (0.0 MB) [deleted]\n",
      "  unit_0005.db: 0 items (0.0 MB) [deleted]\n",
      "  unit_0031.db: 390,352 items (727.5 MB) [deleted]\n",
      "  unit_0024.db: 612,026 items (1119.5 MB) [deleted]\n",
      "  unit_0025.db: 611,426 items (1136.0 MB) [deleted]\n",
      "  unit_0023.db: 629,425 items (1196.3 MB) [deleted]\n",
      "  unit_0030.db: 636,598 items (1193.6 MB) [deleted]\n",
      "  unit_0027.db: 638,320 items (1129.6 MB) [deleted]\n",
      "  unit_0022.db: 646,289 items (1200.7 MB) [deleted]\n",
      "  unit_0028.db: 630,484 items (1161.6 MB) [deleted]\n",
      "  unit_0029.db: 644,447 items (1184.6 MB) [deleted]\n",
      "  unit_0026.db: 645,868 items (1125.3 MB) [deleted]\n",
      "  unit_0021.db: 653,387 items (1181.1 MB) [deleted]\n",
      "  unit_0008.db: 742,716 items (1174.5 MB) [deleted]\n",
      "  unit_0013.db: 747,286 items (1185.5 MB) [deleted]\n",
      "  unit_0020.db: 683,342 items (1161.0 MB) [deleted]\n",
      "  unit_0017.db: 764,519 items (1195.6 MB) [deleted]\n",
      "  unit_0012.db: 783,286 items (1264.0 MB) [deleted]\n",
      "  unit_0011.db: 747,252 items (1214.2 MB) [deleted]\n",
      "  unit_0015.db: 828,745 items (1266.1 MB) [deleted]\n",
      "  unit_0019.db: 834,983 items (1322.6 MB) [deleted]\n",
      "  unit_0016.db: 796,774 items (1257.1 MB) [deleted]\n",
      "  unit_0014.db: 813,163 items (1263.4 MB) [deleted]\n",
      "  unit_0009.db: 895,738 items (1444.7 MB) [deleted]\n",
      "  unit_0018.db: 906,229 items (1403.7 MB) [deleted]\n",
      "  unit_0010.db: 868,328 items (1405.3 MB) [deleted]\n",
      "\n",
      "Phase 3: Finalizing (flush)...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Performing final flush...\n",
      "Running post-ingestion compaction...\n",
      "\n",
      "INCREMENTAL COMPACTION (CACHE-BASED)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-10-04 14:26:20\n",
      "\n",
      "Configuration\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DB path:           ...t/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams_processed.db\n",
      "Work units file:   1grams.db.work_units.json\n",
      "Estimated keys:    41,783,242\n",
      "Total units:       32\n",
      "Units remaining:   32\n",
      "Checkpoint file:   1grams.db.compaction.checkpoint\n",
      "Initial DB size:   26.60 GB\n",
      "\n",
      "Progress\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[unit_0031     25.9s]:  100%|██████████████████████████████████████████████████| 32/32 [08:23<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Incremental Compaction Summary\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Units compacted this run:    32\n",
      "Total units completed:       32/32\n",
      "Size before:                 26.60 GB\n",
      "Size after:                  21.96 GB\n",
      "Space saved:                 4.64 GB (17.4%)\n",
      "Total runtime:               0:08:23\n",
      "Time per unit:               0:00:15\n",
      "Units per hour:              228.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Generating output whitelist...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "  Output path: ...LP_corpora/Google_Books/20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  Extracting top 40,000 tokens\n",
      "  Generated whitelist with 40,000 tokens in 200.7s\n",
      "\n",
      "╭───────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ PROCESSING COMPLETE: Final DB contains 17,150,983 items, 29,188.0 MB                  │\n",
      "│ DB: ...t/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams_processed.db │\n",
      "│ Whitelist: ...Google_Books/20200217/eng/1gram_files/1grams_processed.db/whitelist.txt │\n",
      "╰───────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:38:05.044665Z",
     "start_time": "2025-10-04T18:38:05.042160Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f9ca933e73eda985",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist_w2v (Singularity)",
   "language": "python",
   "name": "hist_w2v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
