{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T00:51:54.571849Z",
     "start_time": "2025-09-25T00:51:43.900815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%cd /scratch/edk202/ngram-prep\n",
    "\n",
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "\n",
    "%pip install -e . --no-build-isolation -q"
   ],
   "id": "1ad267332e4b1fcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/edk202/ngram-prep\n",
      "env: LC_ALL=C.UTF-8\n",
      "env: LANG=C.UTF-8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T00:51:56.101126Z",
     "start_time": "2025-09-25T00:51:56.060140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Auto-reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "ad3fc5fc588e8363",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T00:51:57.195294Z",
     "start_time": "2025-09-25T00:51:57.159840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standard stuff\n",
    "from pathlib import Path\n",
    "\n",
    "# NLTK stuff\n",
    "from nltk.corpus import stopwords; stopwords = set(stopwords.words(\"english\"))\n",
    "from nltk.stem import WordNetLemmatizer; lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Raw n-gram acquisition stuff\n",
    "from ngram_acquire.pipeline.orchestrate import download_and_ingest_to_rocksdb\n",
    "from ngram_acquire.pipeline.logger import setup_logger\n",
    "\n",
    "# Cython utilities\n",
    "from ngram_filter.config import PipelineConfig, FilterConfig\n",
    "from ngram_filter.pipeline.orchestrator import build_processed_db"
   ],
   "id": "be55f147a8e96bcf",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T00:51:58.869227Z",
     "start_time": "2025-09-25T00:51:58.785056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "setup_logger(\n",
    "    db_path=\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams.db\",\n",
    "    console=False,\n",
    "    rotate=True,\n",
    "    max_bytes=100_000_000,\n",
    "    backup_count=5,\n",
    "    force=True\n",
    ")"
   ],
   "id": "5b072e6dbd0ce17c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams.db/ngram_download_20250924_205158.log')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Download Unigrams and Ingest to a RocksDB Database",
   "id": "fe5d0309548b386a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T17:53:57.643845Z",
     "start_time": "2025-09-24T17:23:08.181927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size = 1,\n",
    "    repo_release_id = \"20200217\",\n",
    "    repo_corpus_id = \"eng\",\n",
    "    db_path = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams.db\",\n",
    "    file_range = (0, 23),\n",
    "    random_seed = 21,\n",
    "    workers = 30,\n",
    "    use_threads = False,\n",
    "    ngram_type = \"tagged\",\n",
    "    overwrite = True,\n",
    "    write_batch_size = 100_000,\n",
    "    open_type = \"write:packed24\",\n",
    "    post_compact = True\n",
    ")"
   ],
   "id": "9f25804d1183f1fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mStart Time: 2025-09-24 13:23:08\u001B[0m\n",
      "\u001B[4m\n",
      "Download & Ingestion Configuration\u001B[0m\n",
      "Ngram repository:           https://storage.googleapis.com/books/ngrams/books/20200217/eng/eng-1-ngrams_exports.html\n",
      "RocksDB database path:      /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams.db\n",
      "File index range:           0 to 23 (count ~ 24)\n",
      "Total files available:      24\n",
      "Files to process:           24\n",
      "First file URL:             http://storage.googleapis.com/books/ngrams/books/20200217/eng/1-00012-of-00024.gz\n",
      "Last file URL:              http://storage.googleapis.com/books/ngrams/books/20200217/eng/1-00005-of-00024.gz\n",
      "Ngram size:                 1\n",
      "Ngram filtering:            tagged\n",
      "Overwrite mode:             True\n",
      "Write batch size:           100,000\n",
      "Worker processes/threads:   30 (processes)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|\u001B[34m██████████\u001B[0m| 24/24 [09:27<00:00, 23.67s/files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[33mStarting post-ingest compaction...\u001B[0m\n",
      "\u001B[32mCompaction completed in 0:21:19.653414\u001B[0m\n",
      "\u001B[32m\n",
      "Processing completed!\u001B[0m\n",
      "Fully processed files: 24\n",
      "Total entries written: 41,783,218\n",
      "Write batches flushed: 24\n",
      "Uncompressed data processed: 43.28 GB\n",
      "Processing throughput: 23.96 MB/sec\n",
      "\u001B[31m\n",
      "End Time: 2025-09-24 13:53:57.637803\u001B[0m\n",
      "\u001B[31mTotal Runtime: 0:30:49.421663\u001B[0m\n",
      "\u001B[34m\n",
      "Time per file: 0:01:17.059236\u001B[0m\n",
      "\u001B[34mFiles per hour: 46.7\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Processing Pipeline",
   "id": "97d09d26c6f2043f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T01:02:42.457691Z",
     "start_time": "2025-09-25T00:52:03.410697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_db = Path(\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams.db\")\n",
    "dst_db = src_db.parent / \"1grams_processed.db\"\n",
    "tmp_dir = src_db.parent / \"processing_tmp\"\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    src_db=src_db,\n",
    "    dst_db=dst_db,\n",
    "    tmp_dir=tmp_dir,\n",
    "    readers=8,\n",
    "    ingestors=16,\n",
    "    partitioning_sample_rate=0.001,\n",
    "    mode=\"restart\",\n",
    "    delete_after_ingest=True,\n",
    "    progress_every_s=60.0,\n",
    "    output_whitelist_path=dst_db / \"whitelist.txt\",\n",
    "    output_whitelist_top_n=40_000\n",
    ")\n",
    "\n",
    "filter_config = FilterConfig(\n",
    "    stop_set=stopwords,\n",
    "    lemma_gen=lemmatizer,\n",
    ")\n",
    "\n",
    "build_processed_db(pipeline_config, filter_config)"
   ],
   "id": "57db436f73cba01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "Configuration:\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "  Workers: 8\n",
      "  Work units: 64\n",
      "  Source: /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams.db\n",
      "  Destination: 02/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams_processed.db\n",
      "  Buffer: 100,000 items, 128MB\n",
      "  Profile: write:packed24\n",
      "  Input whitelist: None\n",
      "  Output whitelist: .../20200217/eng/5gram_files/1grams_processed.db/whitelist.txt (top 40,000 keys)\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "  Clean restart - resampling and creating new work units\n",
      "  Sampling database at 0.00100 rate (prefix_length=2)...\n",
      "  Targeting 41,783 samples using reservoir sampling\n",
      "  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "  RESERVOIR SAMPLING CONFIGURATION\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  Target sample size:     41,783 items\n",
      "  Progress interval:      10,000,000 items\n",
      "  Database limit:         No limit (full traversal)\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  Processed 10,000,000 items\n",
      "  Processed 20,000,000 items\n",
      "  Processed 30,000,000 items\n",
      "  Processed 40,000,000 items\n",
      "  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "  RESERVOIR SAMPLING RESULTS\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  Items processed:        41,780,822\n",
      "  Metadata entries:       2,420\n",
      "  Final sample size:      41,783\n",
      "  Execution time:         118.8731 seconds\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  PERFORMANCE METRICS\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  Processing rate:        351,474 items/second\n",
      "  Time per item:          2.85 microseconds\n",
      "  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "  Sampling complete: 41,783 samples collected, 2752 unique prefixes\n",
      "  Creating 64 work units targeting 652,859 records each\n",
      "    Unit 0: start → 303a (~653,000 records)\n",
      "    Unit 1: 303a → 312d (~788,000 records)\n",
      "    Unit 2: 312d → 3133 (~765,000 records)\n",
      "    Unit 3: 3133 → 3138 (~778,000 records)\n",
      "    Unit 4: 3138 → 322d (~803,000 records)\n",
      "    Unit 5: 322d → 3235 (~756,000 records)\n",
      "    Unit 6: 3235 → 332d (~740,000 records)\n",
      "    Unit 7: 332d → 3337 (~683,000 records)\n",
      "    Unit 8: 3337 → 3434 (~678,000 records)\n",
      "    Unit 9: 3434 → 3533 (~703,000 records)\n",
      "    Unit 10: 3533 → 3632 (~656,000 records)\n",
      "    Unit 11: 3632 → 3733 (~661,000 records)\n",
      "    Unit 12: 3733 → 3834 (~684,000 records)\n",
      "    Unit 13: 3834 → 3936 (~665,000 records)\n",
      "    Unit 14: 3936 → 4165 (~683,000 records)\n",
      "    Unit 15: 4165 → 4177 (~659,000 records)\n",
      "    Unit 16: 4177 → 426d (~672,000 records)\n",
      "    Unit 17: 426d → 4355 (~655,000 records)\n",
      "    Unit 18: 4355 → 4370 (~841,000 records)\n",
      "    Unit 19: 4370 → 446a (~685,000 records)\n",
      "    Unit 20: 446a → 4575 (~672,000 records)\n",
      "    Unit 21: 4575 → 4745 (~655,000 records)\n",
      "    Unit 22: 4745 → 4842 (~659,000 records)\n",
      "    Unit 23: 4842 → 4935 (~656,000 records)\n",
      "    Unit 24: 4935 → 4a4f (~653,000 records)\n",
      "    Unit 25: 4a4f → 4b70 (~723,000 records)\n",
      "    Unit 26: 4b70 → 4c7a (~665,000 records)\n",
      "    Unit 27: 4c7a → 4d66 (~706,000 records)\n",
      "    Unit 28: 4d66 → 4e66 (~710,000 records)\n",
      "    Unit 29: 4e66 → 5044 (~654,000 records)\n",
      "    Unit 30: 5044 → 5070 (~736,000 records)\n",
      "    Unit 31: 5070 → 5266 (~785,000 records)\n",
      "    Unit 32: 5266 → 5362 (~760,000 records)\n",
      "    Unit 33: 5362 → 5373 (~655,000 records)\n",
      "    Unit 34: 5373 → 5466 (~681,000 records)\n",
      "    Unit 35: 5466 → 5644 (~654,000 records)\n",
      "    Unit 36: 5644 → 576a (~702,000 records)\n",
      "    Unit 37: 576a → 6165 (~713,000 records)\n",
      "    Unit 38: 6165 → 6173 (~656,000 records)\n",
      "    Unit 39: 6173 → 6270 (~679,000 records)\n",
      "    Unit 40: 6270 → 636f (~655,000 records)\n",
      "    Unit 41: 636f → 6379 (~653,000 records)\n",
      "    Unit 42: 6379 → 6470 (~696,000 records)\n",
      "    Unit 43: 6470 → 6574 (~655,000 records)\n",
      "    Unit 44: 6574 → 6673 (~708,000 records)\n",
      "    Unit 45: 6673 → 686a (~712,000 records)\n",
      "    Unit 46: 686a → 696f (~914,000 records)\n",
      "    Unit 47: 696f → 6c62 (~657,000 records)\n",
      "    Unit 48: 6c62 → 6d62 (~697,000 records)\n",
      "    Unit 49: 6d62 → 6e66 (~722,000 records)\n",
      "    Unit 50: 6e66 → 6f71 (~666,000 records)\n",
      "    Unit 51: 6f71 → 7069 (~666,000 records)\n",
      "    Unit 52: 7069 → 7075 (~655,000 records)\n",
      "    Unit 53: 7075 → 7266 (~668,000 records)\n",
      "    Unit 54: 7266 → 7366 (~665,000 records)\n",
      "    Unit 55: 7366 → 7375 (~702,000 records)\n",
      "    Unit 56: 7375 → 7469 (~729,000 records)\n",
      "    Unit 57: 7469 → 756f (~671,000 records)\n",
      "    Unit 58: 756f → 7770 (~660,000 records)\n",
      "  Created 59 balanced work units\n",
      "  Work unit balance analysis:\n",
      "    Average: 697,169 records per unit\n",
      "    Range: 653,000 to 914,000\n",
      "    Ratio: 1.4x difference\n",
      "  Sorted 59 work units by size (largest first) for optimal parallelization\n",
      "\n",
      "Phase 2: Processing 59 work units with 8 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    recs scanned   recs written   items kept     throughput     elapsed\n",
      "   ─────────────────────────────────────────────────────────────────────\n",
      "    25,340,312     12,021,287     49%            421.7k/s       1m00s       \n",
      "   ──────────────────────────────  final  ──────────────────────────────\n",
      "    41,149,009     24,963,618     61%            375.8k/s       1m49s       \n",
      "\n",
      "Phase 3: Merging worker outputs into final database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "  Found 59 worker output files\n",
      "  Batch size: 128MB, 200,000 items\n",
      "  Parallel readers: 16\n",
      "  Delete shards: Yes (after successful ingestion)\n",
      "  Folding 59 shard(s) with 16 parallel readers...\n",
      "  unit_0001.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0001.db\n",
      "  unit_0003.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0003.db\n",
      "  unit_0000.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0000.db\n",
      "  unit_0002.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0002.db\n",
      "  unit_0004.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0004.db\n",
      "  unit_0006.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0006.db\n",
      "  unit_0005.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0005.db\n",
      "  unit_0008.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0008.db\n",
      "  unit_0007.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0007.db\n",
      "  unit_0011.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0011.db\n",
      "  unit_0010.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0010.db\n",
      "  unit_0009.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0009.db\n",
      "  unit_0013.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0013.db\n",
      "  unit_0012.db: 0 items (0.0 MB)\n",
      "  Deleted processed shard: unit_0012.db\n",
      "  unit_0014.db: 227,215 items (345.2 MB)\n",
      "  Deleted processed shard: unit_0014.db\n",
      "  unit_0024.db: 328,872 items (545.2 MB)\n",
      "  Deleted processed shard: unit_0024.db\n",
      "  unit_0021.db: 376,676 items (612.4 MB)\n",
      "  Deleted processed shard: unit_0021.db\n",
      "  unit_0029.db: 385,067 items (593.3 MB)\n",
      "  Deleted processed shard: unit_0029.db\n",
      "  unit_0023.db: 401,719 items (656.5 MB)\n",
      "  Deleted processed shard: unit_0023.db\n",
      "  unit_0022.db: 401,081 items (637.7 MB)\n",
      "  Deleted processed shard: unit_0022.db\n",
      "  unit_0019.db: 400,993 items (633.9 MB)\n",
      "  Deleted processed shard: unit_0019.db\n",
      "  unit_0017.db: 419,552 items (665.9 MB)\n",
      "  Deleted processed shard: unit_0017.db\n",
      "  unit_0016.db: 423,352 items (675.9 MB)\n",
      "  Deleted processed shard: unit_0016.db\n",
      "  unit_0020.db: 429,836 items (692.1 MB)\n",
      "  Deleted processed shard: unit_0020.db\n",
      "  unit_0026.db: 432,469 items (691.0 MB)\n",
      "  Deleted processed shard: unit_0026.db\n",
      "  unit_0027.db: 438,679 items (686.6 MB)\n",
      "  Deleted processed shard: unit_0027.db\n",
      "  unit_0015.db: 463,288 items (738.8 MB)\n",
      "  Deleted processed shard: unit_0015.db\n",
      "  unit_0028.db: 473,260 items (706.8 MB)\n",
      "  Deleted processed shard: unit_0028.db\n",
      "  unit_0025.db: 484,582 items (731.0 MB)\n",
      "  Deleted processed shard: unit_0025.db\n",
      "  unit_0018.db: 603,448 items (999.6 MB)\n",
      "  Deleted processed shard: unit_0018.db\n",
      "  unit_0040.db: 345,524 items (627.3 MB)\n",
      "  Deleted processed shard: unit_0040.db\n",
      "  unit_0039.db: 318,354 items (575.6 MB)\n",
      "  Deleted processed shard: unit_0039.db\n",
      "  unit_0030.db: 496,401 items (783.3 MB)\n",
      "  Deleted processed shard: unit_0030.db\n",
      "  unit_0038.db: 319,566 items (583.5 MB)\n",
      "  Deleted processed shard: unit_0038.db\n",
      "  unit_0043.db: 298,218 items (556.2 MB)\n",
      "  Deleted processed shard: unit_0043.db\n",
      "  unit_0045.db: 358,030 items (644.8 MB)\n",
      "  Deleted processed shard: unit_0045.db\n",
      "  unit_0037.db: 361,812 items (581.2 MB)\n",
      "  Deleted processed shard: unit_0037.db\n",
      "  unit_0042.db: 338,313 items (608.5 MB)\n",
      "  Deleted processed shard: unit_0042.db\n",
      "  unit_0041.db: 321,454 items (616.4 MB)\n",
      "  Deleted processed shard: unit_0041.db\n",
      "  unit_0044.db: 342,532 items (667.9 MB)\n",
      "  Deleted processed shard: unit_0044.db\n",
      "  unit_0036.db: 426,834 items (687.8 MB)\n",
      "  Deleted processed shard: unit_0036.db\n",
      "  unit_0035.db: 451,764 items (710.7 MB)\n",
      "  Deleted processed shard: unit_0035.db\n",
      "  unit_0032.db: 465,121 items (712.7 MB)\n",
      "  Deleted processed shard: unit_0032.db\n",
      "  unit_0033.db: 506,531 items (793.5 MB)\n",
      "  Deleted processed shard: unit_0033.db\n",
      "  unit_0031.db: 454,329 items (717.6 MB)\n",
      "  Deleted processed shard: unit_0031.db\n",
      "  unit_0056.db: 341,211 items (623.0 MB)\n",
      "  Deleted processed shard: unit_0056.db\n",
      "  unit_0054.db: 304,605 items (566.2 MB)\n",
      "  Deleted processed shard: unit_0054.db\n",
      "  unit_0055.db: 318,127 items (580.0 MB)\n",
      "  Deleted processed shard: unit_0055.db\n",
      "  unit_0034.db: 470,293 items (722.5 MB)\n",
      "  Deleted processed shard: unit_0034.db\n",
      "  unit_0053.db: 311,551 items (595.5 MB)\n",
      "  Deleted processed shard: unit_0053.db\n",
      "  unit_0058.db: 303,256 items (576.9 MB)\n",
      "  Deleted processed shard: unit_0058.db\n",
      "  unit_0046.db: 390,171 items (741.8 MB)\n",
      "  Deleted processed shard: unit_0046.db\n",
      "  unit_0057.db: 332,738 items (636.0 MB)\n",
      "  Deleted processed shard: unit_0057.db\n",
      "  unit_0052.db: 327,632 items (582.1 MB)\n",
      "  Deleted processed shard: unit_0052.db\n",
      "  unit_0051.db: 313,492 items (553.1 MB)\n",
      "  Deleted processed shard: unit_0051.db\n",
      "  unit_0048.db: 336,958 items (616.6 MB)\n",
      "  Deleted processed shard: unit_0048.db\n",
      "  unit_0047.db: 309,123 items (545.2 MB)\n",
      "  Deleted processed shard: unit_0047.db\n",
      "  unit_0049.db: 367,657 items (622.1 MB)\n",
      "  Deleted processed shard: unit_0049.db\n",
      "  unit_0050.db: 328,902 items (585.7 MB)\n",
      "  Deleted processed shard: unit_0050.db\n",
      "\n",
      "Phase 3: Finalizing...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "  Performing final flush...\n",
      "  Compacting...\n",
      "\n",
      "Phase 4: Generating output whitelist...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "  Output path: .../Google_Books/20200217/eng/5gram_files/1grams_processed.db/whitelist.txt\n",
      "  Extracting top 40,000 tokens\n",
      "  Generated whitelist with 40,000 tokens in 161.9s\n",
      "\n",
      "╭─────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ PROCESSING COMPLETE: Final DB contains 17,250,588 items, 29,301.5 MB                │\n",
      "│ DB: ...02/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams_processed.db     │\n",
      "│ Whitelist: ...Books/20200217/eng/5gram_files/1grams_processed.db/whitelist.txt      │\n",
      "╰─────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c29166b79e15ab2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist_w2v (Singularity)",
   "language": "python",
   "name": "hist_w2v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
