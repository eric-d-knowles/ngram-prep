{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Setup**\n",
    "## Recompile Cython Extensions"
   ],
   "id": "b784e1a1cda09447"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%cd /scratch/edk202/ngram-prep\n",
    "\n",
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "\n",
    "%pip install -e . --no-build-isolation -q"
   ],
   "id": "1ad267332e4b1fcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "a8e3ef360e1fe339"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T03:38:59.767701Z",
     "start_time": "2025-10-10T03:38:59.666255Z"
    }
   },
   "cell_type": "code",
   "source": "# Auto-reload edited scripts\n%load_ext autoreload\n%autoreload 2\n\n# NLTK resources\nfrom nltk.corpus import stopwords; stopwords = set(stopwords.words(\"english\"))\nfrom nltk.stem import WordNetLemmatizer; lemmatizer = WordNetLemmatizer()\n\n# Ngram acquisition functions\nfrom ngram_prep.ngram_acquire import download_and_ingest_to_rocksdb\nfrom ngram_prep.ngram_acquire.logger import setup_logger\n\n# Ngram processing functions\nfrom pathlib import Path\nfrom ngram_prep.ngram_filter.config import PipelineConfig, FilterConfig\nfrom ngram_prep.ngram_filter.pipeline.orchestrator import build_processed_db",
   "id": "1998c9fafc32aa8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up logging to file",
   "id": "abd0247891250279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T03:39:04.852369Z",
     "start_time": "2025-10-10T03:39:04.714328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "setup_logger(\n",
    "    db_path=\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams.db\",\n",
    "    console=False,\n",
    "    rotate=True,\n",
    "    max_bytes=100_000_000,\n",
    "    backup_count=5,\n",
    "    force=True\n",
    ")"
   ],
   "id": "34a1e1724ae74cc1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams.db/ngram_download_20251009_233904.log')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Download Unigrams and Ingest to RocksDB**",
   "id": "879927f391b65d8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=1,\n",
    "    repo_release_id=\"20200217\",\n",
    "    repo_corpus_id=\"eng\",\n",
    "    db_path_stub=\"/vast/edk202/NLP_corpora/Google_Books/\",\n",
    "    file_range=(0, 23),\n",
    "    random_seed=98,\n",
    "    workers=25,\n",
    "    use_threads=False,\n",
    "    ngram_type=\"tagged\",\n",
    "    overwrite_db=True,\n",
    "    write_batch_size=100_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True\n",
    ")"
   ],
   "id": "23449b75707ae1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Run Processing Pipeline**",
   "id": "97d09d26c6f2043f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T03:46:24.782329Z",
     "start_time": "2025-10-10T03:39:10.339382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_db = Path(\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams.db\")\n",
    "dst_db = src_db.parent / \"1grams_processed.db\"\n",
    "tmp_dir = src_db.parent / \"processing_tmp\"\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    src_db=src_db,\n",
    "    dst_db=dst_db,\n",
    "    tmp_dir=tmp_dir,\n",
    "    num_workers=32,\n",
    "    mode=\"restart\",\n",
    "    enable_ingest=True,\n",
    "    num_ingest_workers=32,\n",
    "    delete_after_ingest=True,\n",
    "    compact_after_ingest=True,\n",
    "    progress_every_s=60.0,\n",
    "    output_whitelist_path=dst_db / \"whitelist.txt\",\n",
    "    output_whitelist_top_n=40_000\n",
    ")\n",
    "\n",
    "filter_config = FilterConfig(\n",
    "    stop_set=stopwords,\n",
    "    lemma_gen=lemmatizer,\n",
    ")\n",
    "\n",
    "build_processed_db(pipeline_config, filter_config)"
   ],
   "id": "57db436f73cba01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\u001B[4mPipeline\u001B[0m\n",
      "Run mode:               restart\n",
      "Ingest after filtering: True\n",
      "Compact after ingest:   True\n",
      "\n",
      "\u001B[4mWorkers\u001B[0m\n",
      "Num Workers:        32\n",
      "Initial work units: 32\n",
      "Dynamic splitting:  Enabled\n",
      "Profiles:           read=read:packed24, write=write:packed24\n",
      "Buffer:             100,000 items, 128.00 MB\n",
      "\n",
      "\u001B[4mFiles\u001B[0m\n",
      "Source: /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams.db\n",
      "Destination: /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams_processed.db\n",
      "Input whitelist: None\n",
      "Output whitelist: ...ks/20200217/eng/1gram_files/1grams_processed.db/whitelist.txt (top 40,000 keys)\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Created 32 uniform work units (byte-range partitioning)\n",
      "Dynamic splitting will handle load balancing at runtime\n",
      "\n",
      "Phase 2: Processing 32 work units with 32 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    recs scanned     recs written     items kept       throughput       elapsed\n",
      "   ───────────────────────────────────────────────────────────────────────────────\n",
      "    6,224,061        0                21%              102.9k/s         1m00s         \n",
      "    29,966,209       0                53%              248.7k/s         2m00s         \n",
      "   ──────────────────────────────────── final ────────────────────────────────────\n",
      "    41,783,242       0                60%              259.9k/s         2m40s         \n",
      "\n",
      "Phase 3: Merging worker outputs into final database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ingesting shards with batch size 128MB / 200,000 items\n",
      "Deleting shards after successful ingestion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|█████████████████████████████████████████████████████████| 74/74 [01:55<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 3: Finalizing (flush)...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Performing final flush...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|█████████████████████████████████████████████████████████| 74/74 [02:06<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Ingestion Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         25.15 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compaction completed in 0:02:26\n",
      "Size before:             25.15 GB\n",
      "Size after:              21.91 GB\n",
      "Space saved:             3.24 GB (12.9%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                          │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 16,500,108                                                                            │\n",
      "│ Size: 26.31 GB                                                                               │\n",
      "│ Database: /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams_processed.db │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5c9a9336598ef9f9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist_w2v (Singularity)",
   "language": "python",
   "name": "hist_w2v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
