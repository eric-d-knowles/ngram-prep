{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:59:21.715428Z",
     "start_time": "2025-09-07T22:59:11.711636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%cd /scratch/edk202/ngram-prep\n",
    "\n",
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "\n",
    "%pip install -e . --no-build-isolation -q"
   ],
   "id": "e3ca08e9c2636151",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/edk202/ngram-prep\n",
      "env: LC_ALL=C.UTF-8\n",
      "env: LANG=C.UTF-8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "344907c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:59:22.133415Z",
     "start_time": "2025-09-07T22:59:21.726298Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pytest"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:59:25.274882Z",
     "start_time": "2025-09-07T22:59:22.704376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pytest.main([\n",
    "    \"-q\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/common_db/test_api.py\",\n",
    "\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/db/test_metadata.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/db/test_write.py\",\n",
    "\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/io/test_download.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/io/test_fetch.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/io/test_locations.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/io/test_parse.py\",\n",
    "\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/pipeline/test_logger.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/pipeline/test_orchestrate.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/pipeline/test_report.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/pipeline/test_runner.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/pipeline/test_worker.py\",\n",
    "\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/utils/test_cleanup.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/utils/test_filters.py\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/utils/test_vocab.py\"\n",
    "])"
   ],
   "id": "ba03bdb6a91f50a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[31mF\u001B[0m\u001B[32m.\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[31mF\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31m [ 72%]\n",
      "\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[31mF\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[31m                                              [100%]\u001B[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001B[31m\u001B[1m__________________ TestOpenDb.test_open_db_bulk_finalization ___________________\u001B[0m\n",
      "\n",
      "self = <test_api.TestOpenDb object at 0x14affa10b110>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_open_db_bulk_finalization\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"Test bulk profile with finalization\"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mwith\u001B[39;49;00m tempfile.TemporaryDirectory() \u001B[94mas\u001B[39;49;00m tmpdir:\u001B[90m\u001B[39;49;00m\n",
      "            db_path = Path(tmpdir) / \u001B[33m\"\u001B[39;49;00m\u001B[33mtest.db\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">           \u001B[94mwith\u001B[39;49;00m open_db(db_path, profile=\u001B[33m\"\u001B[39;49;00m\u001B[33mbulk\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, finalize_bulk_to=\u001B[33m\"\u001B[39;49;00m\u001B[33mread\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m) \u001B[94mas\u001B[39;49;00m db:\u001B[90m\u001B[39;49;00m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/common_db/test_api.py\u001B[0m:60: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001B[1m\u001B[31m/ext3/miniforge3/envs/hist_w2v/lib/python3.11/contextlib.py\u001B[0m:137: in __enter__\n",
      "    \u001B[0m\u001B[94mreturn\u001B[39;49;00m \u001B[96mnext\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m.gen)\u001B[90m\u001B[39;49;00m\n",
      "           ^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = PosixPath('/state/partition1/job-65980021/tmpnnarbpcr/test.db')\n",
      "mode = 'rw', profile = 'bulk', finalize_on_exit = False, compact_on_exit = False\n",
      "open_kwargs = {'finalize_bulk_to': 'read'}\n",
      "p = '/state/partition1/job-65980021/tmpnnarbpcr/test.db', read_only = False\n",
      "prof = 'bulk'\n",
      "\n",
      "    \u001B[0m\u001B[37m@contextmanager\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mopen_db\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "        path: PathLike,\u001B[90m\u001B[39;49;00m\n",
      "        *,\u001B[90m\u001B[39;49;00m\n",
      "        mode: Mode = \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        profile: Optional[Profile] = \u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        finalize_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,   \u001B[90m# optional: handy for bulk_write\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        compact_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,    \u001B[90m# optional: force a compact at the end\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        **open_kwargs: Any,\u001B[90m\u001B[39;49;00m\n",
      "    ) -> Iterator[rs.DB]:\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "    \u001B[33m    Thin entry to rocks_shim.open(...).\u001B[39;49;00m\n",
      "    \u001B[33m\u001B[39;49;00m\n",
      "    \u001B[33m    mode: \"ro\" | \"rw\"\u001B[39;49;00m\n",
      "    \u001B[33m    profile: \"read\" | \"write\" | \"bulk\" | \"bulk_write\" (default: 'read' if ro else 'write')\u001B[39;49;00m\n",
      "    \u001B[33m    \"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        p = \u001B[96mstr\u001B[39;49;00m(path)\u001B[90m\u001B[39;49;00m\n",
      "        read_only = (mode == \u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        prof: Profile = profile \u001B[95mor\u001B[39;49;00m (\u001B[33m\"\u001B[39;49;00m\u001B[33mread\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mwrite\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       db = rs.open(\u001B[90m\u001B[39;49;00m\n",
      "            p,\u001B[90m\u001B[39;49;00m\n",
      "            mode=\u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            profile=prof,\u001B[90m\u001B[39;49;00m\n",
      "            create_if_missing=\u001B[95mnot\u001B[39;49;00m read_only,\u001B[90m\u001B[39;49;00m\n",
      "            **open_kwargs,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       TypeError: open(): incompatible function arguments. The following argument types are supported:\u001B[0m\n",
      "\u001B[1m\u001B[31mE           1. (path: str, *, mode: str = 'rw', profile: str = 'write', create_if_missing: bool = True) -> rocks_shim.rocks_shim.DB\u001B[0m\n",
      "\u001B[1m\u001B[31mE       \u001B[0m\n",
      "\u001B[1m\u001B[31mE       Invoked with: '/state/partition1/job-65980021/tmpnnarbpcr/test.db'; kwargs: mode='rw', profile='bulk', create_if_missing=True, finalize_bulk_to='read'\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31msrc/common_db/api.py\u001B[0m:34: TypeError\n",
      "\u001B[31m\u001B[1m__________________ TestOpenDb.test_open_db_exception_handling __________________\u001B[0m\n",
      "\n",
      "self = <test_api.TestOpenDb object at 0x14affa10a550>\n",
      "mock_db_open = <MagicMock name='open' id='22745371935888'>\n",
      "\n",
      "    \u001B[0m\u001B[37m@patch\u001B[39;49;00m(\u001B[33m'\u001B[39;49;00m\u001B[33mcommon_db.api.rs.DB.open\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_open_db_exception_handling\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m, mock_db_open):\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"Test that exceptions during finalization don't crash\"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Create a mock DB that raises exceptions on finalization methods\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        mock_db = Mock()\u001B[90m\u001B[39;49;00m\n",
      "        mock_db.finalize_bulk.side_effect = \u001B[96mException\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mFinalize failed\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        mock_db.compact_all.side_effect = \u001B[96mException\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mCompact failed\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        mock_db.set_profile.side_effect = \u001B[96mException\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mSet profile failed\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        mock_db.close.side_effect = \u001B[96mException\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mClose failed\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        mock_db_open.return_value = mock_db\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Should not raise exceptions despite failures in finalization\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94mwith\u001B[39;49;00m open_db(\u001B[33m\"\u001B[39;49;00m\u001B[33m/fake/path\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, profile=\u001B[33m\"\u001B[39;49;00m\u001B[33mbulk\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/common_db/test_api.py\u001B[0m:100: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001B[1m\u001B[31m/ext3/miniforge3/envs/hist_w2v/lib/python3.11/contextlib.py\u001B[0m:137: in __enter__\n",
      "    \u001B[0m\u001B[94mreturn\u001B[39;49;00m \u001B[96mnext\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m.gen)\u001B[90m\u001B[39;49;00m\n",
      "           ^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/fake/path', mode = 'rw', profile = 'bulk', finalize_on_exit = False\n",
      "compact_on_exit = False, open_kwargs = {}, p = '/fake/path', read_only = False\n",
      "prof = 'bulk'\n",
      "\n",
      "    \u001B[0m\u001B[37m@contextmanager\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mopen_db\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "        path: PathLike,\u001B[90m\u001B[39;49;00m\n",
      "        *,\u001B[90m\u001B[39;49;00m\n",
      "        mode: Mode = \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        profile: Optional[Profile] = \u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        finalize_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,   \u001B[90m# optional: handy for bulk_write\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        compact_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,    \u001B[90m# optional: force a compact at the end\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        **open_kwargs: Any,\u001B[90m\u001B[39;49;00m\n",
      "    ) -> Iterator[rs.DB]:\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "    \u001B[33m    Thin entry to rocks_shim.open(...).\u001B[39;49;00m\n",
      "    \u001B[33m\u001B[39;49;00m\n",
      "    \u001B[33m    mode: \"ro\" | \"rw\"\u001B[39;49;00m\n",
      "    \u001B[33m    profile: \"read\" | \"write\" | \"bulk\" | \"bulk_write\" (default: 'read' if ro else 'write')\u001B[39;49;00m\n",
      "    \u001B[33m    \"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        p = \u001B[96mstr\u001B[39;49;00m(path)\u001B[90m\u001B[39;49;00m\n",
      "        read_only = (mode == \u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        prof: Profile = profile \u001B[95mor\u001B[39;49;00m (\u001B[33m\"\u001B[39;49;00m\u001B[33mread\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mwrite\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       db = rs.open(\u001B[90m\u001B[39;49;00m\n",
      "            p,\u001B[90m\u001B[39;49;00m\n",
      "            mode=\u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            profile=prof,\u001B[90m\u001B[39;49;00m\n",
      "            create_if_missing=\u001B[95mnot\u001B[39;49;00m read_only,\u001B[90m\u001B[39;49;00m\n",
      "            **open_kwargs,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       RuntimeError: IO error: No such file or directory: While mkdir if missing: /fake/path: No such file or directory\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31msrc/common_db/api.py\u001B[0m:34: RuntimeError\n",
      "\u001B[31m\u001B[1m_______________ TestOpenDb.test_open_db_no_finalization_methods ________________\u001B[0m\n",
      "\n",
      "self = <test_api.TestOpenDb object at 0x14affa10bed0>\n",
      "mock_db_open = <MagicMock name='open' id='22745372858832'>\n",
      "\n",
      "    \u001B[0m\u001B[37m@patch\u001B[39;49;00m(\u001B[33m'\u001B[39;49;00m\u001B[33mcommon_db.api.rs.DB.open\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_open_db_no_finalization_methods\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m, mock_db_open):\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"Test graceful handling when DB doesn't have finalization methods\"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Create a mock DB without finalization methods\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        mock_db = Mock()\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[96mhasattr\u001B[39;49;00m(mock_db, \u001B[33m\"\u001B[39;49;00m\u001B[33mfinalize_bulk\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdel\u001B[39;49;00m mock_db.finalize_bulk\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[96mhasattr\u001B[39;49;00m(mock_db, \u001B[33m\"\u001B[39;49;00m\u001B[33mcompact_all\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdel\u001B[39;49;00m mock_db.compact_all\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[96mhasattr\u001B[39;49;00m(mock_db, \u001B[33m\"\u001B[39;49;00m\u001B[33mset_profile\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdel\u001B[39;49;00m mock_db.set_profile\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[96mhasattr\u001B[39;49;00m(mock_db, \u001B[33m\"\u001B[39;49;00m\u001B[33mclose\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdel\u001B[39;49;00m mock_db.close\u001B[90m\u001B[39;49;00m\n",
      "        mock_db_open.return_value = mock_db\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Should work fine even without these methods\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94mwith\u001B[39;49;00m open_db(\u001B[33m\"\u001B[39;49;00m\u001B[33m/fake/path\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, profile=\u001B[33m\"\u001B[39;49;00m\u001B[33mbulk\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/common_db/test_api.py\u001B[0m:124: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001B[1m\u001B[31m/ext3/miniforge3/envs/hist_w2v/lib/python3.11/contextlib.py\u001B[0m:137: in __enter__\n",
      "    \u001B[0m\u001B[94mreturn\u001B[39;49;00m \u001B[96mnext\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m.gen)\u001B[90m\u001B[39;49;00m\n",
      "           ^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/fake/path', mode = 'rw', profile = 'bulk', finalize_on_exit = False\n",
      "compact_on_exit = False, open_kwargs = {}, p = '/fake/path', read_only = False\n",
      "prof = 'bulk'\n",
      "\n",
      "    \u001B[0m\u001B[37m@contextmanager\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mopen_db\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "        path: PathLike,\u001B[90m\u001B[39;49;00m\n",
      "        *,\u001B[90m\u001B[39;49;00m\n",
      "        mode: Mode = \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        profile: Optional[Profile] = \u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        finalize_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,   \u001B[90m# optional: handy for bulk_write\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        compact_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,    \u001B[90m# optional: force a compact at the end\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        **open_kwargs: Any,\u001B[90m\u001B[39;49;00m\n",
      "    ) -> Iterator[rs.DB]:\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "    \u001B[33m    Thin entry to rocks_shim.open(...).\u001B[39;49;00m\n",
      "    \u001B[33m\u001B[39;49;00m\n",
      "    \u001B[33m    mode: \"ro\" | \"rw\"\u001B[39;49;00m\n",
      "    \u001B[33m    profile: \"read\" | \"write\" | \"bulk\" | \"bulk_write\" (default: 'read' if ro else 'write')\u001B[39;49;00m\n",
      "    \u001B[33m    \"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        p = \u001B[96mstr\u001B[39;49;00m(path)\u001B[90m\u001B[39;49;00m\n",
      "        read_only = (mode == \u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        prof: Profile = profile \u001B[95mor\u001B[39;49;00m (\u001B[33m\"\u001B[39;49;00m\u001B[33mread\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mwrite\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       db = rs.open(\u001B[90m\u001B[39;49;00m\n",
      "            p,\u001B[90m\u001B[39;49;00m\n",
      "            mode=\u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            profile=prof,\u001B[90m\u001B[39;49;00m\n",
      "            create_if_missing=\u001B[95mnot\u001B[39;49;00m read_only,\u001B[90m\u001B[39;49;00m\n",
      "            **open_kwargs,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       RuntimeError: IO error: No such file or directory: While mkdir if missing: /fake/path: No such file or directory\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31msrc/common_db/api.py\u001B[0m:34: RuntimeError\n",
      "\u001B[31m\u001B[1m_____________________ test_open_invocation_shape_read_only _____________________\u001B[0m\n",
      "\n",
      "mock_open = <MagicMock name='open' id='22743933106704'>\n",
      "\n",
      "    \u001B[0m\u001B[37m@patch\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mcommon_db.api.rs.DB.open\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_open_invocation_shape_read_only\u001B[39;49;00m(mock_open):\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"Ensure api passes the expected flags to the shim for RO opens.\"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Simulate returning a minimal object that can be used in a context\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mclass\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[92mDummy\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mclose\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mpass\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92miterator\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[94mclass\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[92mIt\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mseek\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m, *_): \u001B[94mpass\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mvalid\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mreturn\u001B[39;49;00m \u001B[94mFalse\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mnext\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mpass\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mkey\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mreturn\u001B[39;49;00m \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mvalue\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mreturn\u001B[39;49;00m \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[94mreturn\u001B[39;49;00m It()\u001B[90m\u001B[39;49;00m\n",
      "        mock_open.return_value = Dummy()\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94mwith\u001B[39;49;00m open_db(\u001B[33m\"\u001B[39;49;00m\u001B[33m/fake/path\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, mode=\u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/common_db/test_api.py\u001B[0m:484: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001B[1m\u001B[31m/ext3/miniforge3/envs/hist_w2v/lib/python3.11/contextlib.py\u001B[0m:137: in __enter__\n",
      "    \u001B[0m\u001B[94mreturn\u001B[39;49;00m \u001B[96mnext\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m.gen)\u001B[90m\u001B[39;49;00m\n",
      "           ^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/fake/path', mode = 'ro', profile = None, finalize_on_exit = False\n",
      "compact_on_exit = False, open_kwargs = {}, p = '/fake/path', read_only = True\n",
      "prof = 'read'\n",
      "\n",
      "    \u001B[0m\u001B[37m@contextmanager\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mopen_db\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "        path: PathLike,\u001B[90m\u001B[39;49;00m\n",
      "        *,\u001B[90m\u001B[39;49;00m\n",
      "        mode: Mode = \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        profile: Optional[Profile] = \u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        finalize_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,   \u001B[90m# optional: handy for bulk_write\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        compact_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,    \u001B[90m# optional: force a compact at the end\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        **open_kwargs: Any,\u001B[90m\u001B[39;49;00m\n",
      "    ) -> Iterator[rs.DB]:\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "    \u001B[33m    Thin entry to rocks_shim.open(...).\u001B[39;49;00m\n",
      "    \u001B[33m\u001B[39;49;00m\n",
      "    \u001B[33m    mode: \"ro\" | \"rw\"\u001B[39;49;00m\n",
      "    \u001B[33m    profile: \"read\" | \"write\" | \"bulk\" | \"bulk_write\" (default: 'read' if ro else 'write')\u001B[39;49;00m\n",
      "    \u001B[33m    \"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        p = \u001B[96mstr\u001B[39;49;00m(path)\u001B[90m\u001B[39;49;00m\n",
      "        read_only = (mode == \u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        prof: Profile = profile \u001B[95mor\u001B[39;49;00m (\u001B[33m\"\u001B[39;49;00m\u001B[33mread\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mwrite\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       db = rs.open(\u001B[90m\u001B[39;49;00m\n",
      "            p,\u001B[90m\u001B[39;49;00m\n",
      "            mode=\u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            profile=prof,\u001B[90m\u001B[39;49;00m\n",
      "            create_if_missing=\u001B[95mnot\u001B[39;49;00m read_only,\u001B[90m\u001B[39;49;00m\n",
      "            **open_kwargs,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       RuntimeError: IO error: No such file or directory: While opening a file for sequentially reading: /fake/path/CURRENT: No such file or directory\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31msrc/common_db/api.py\u001B[0m:34: RuntimeError\n",
      "\u001B[31m\u001B[1m____________________ test_open_invocation_shape_read_write _____________________\u001B[0m\n",
      "\n",
      "mock_open = <MagicMock name='open' id='22746047231056'>\n",
      "\n",
      "    \u001B[0m\u001B[37m@patch\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mcommon_db.api.rs.DB.open\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_open_invocation_shape_read_write\u001B[39;49;00m(mock_open):\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"Ensure api passes the expected flags to the shim for RW opens.\"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mclass\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[92mDummy\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mclose\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mpass\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mput\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m, *_): \u001B[94mpass\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mget\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m, *_): \u001B[94mreturn\u001B[39;49;00m \u001B[94mNone\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92miterator\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[94mclass\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[92mIt\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mseek\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m, *_): \u001B[94mpass\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mvalid\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mreturn\u001B[39;49;00m \u001B[94mFalse\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mnext\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mpass\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mkey\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mreturn\u001B[39;49;00m \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mvalue\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m): \u001B[94mreturn\u001B[39;49;00m \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[94mreturn\u001B[39;49;00m It()\u001B[90m\u001B[39;49;00m\n",
      "        mock_open.return_value = Dummy()\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       \u001B[94mwith\u001B[39;49;00m open_db(\u001B[33m\"\u001B[39;49;00m\u001B[33m/fake/path\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, mode=\u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m) \u001B[94mas\u001B[39;49;00m db:\u001B[90m\u001B[39;49;00m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/common_db/test_api.py\u001B[0m:507: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001B[1m\u001B[31m/ext3/miniforge3/envs/hist_w2v/lib/python3.11/contextlib.py\u001B[0m:137: in __enter__\n",
      "    \u001B[0m\u001B[94mreturn\u001B[39;49;00m \u001B[96mnext\u001B[39;49;00m(\u001B[96mself\u001B[39;49;00m.gen)\u001B[90m\u001B[39;49;00m\n",
      "           ^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "path = '/fake/path', mode = 'rw', profile = None, finalize_on_exit = False\n",
      "compact_on_exit = False, open_kwargs = {}, p = '/fake/path', read_only = False\n",
      "prof = 'write'\n",
      "\n",
      "    \u001B[0m\u001B[37m@contextmanager\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mopen_db\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "        path: PathLike,\u001B[90m\u001B[39;49;00m\n",
      "        *,\u001B[90m\u001B[39;49;00m\n",
      "        mode: Mode = \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        profile: Optional[Profile] = \u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        finalize_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,   \u001B[90m# optional: handy for bulk_write\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        compact_on_exit: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,    \u001B[90m# optional: force a compact at the end\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        **open_kwargs: Any,\u001B[90m\u001B[39;49;00m\n",
      "    ) -> Iterator[rs.DB]:\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "    \u001B[33m    Thin entry to rocks_shim.open(...).\u001B[39;49;00m\n",
      "    \u001B[33m\u001B[39;49;00m\n",
      "    \u001B[33m    mode: \"ro\" | \"rw\"\u001B[39;49;00m\n",
      "    \u001B[33m    profile: \"read\" | \"write\" | \"bulk\" | \"bulk_write\" (default: 'read' if ro else 'write')\u001B[39;49;00m\n",
      "    \u001B[33m    \"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        p = \u001B[96mstr\u001B[39;49;00m(path)\u001B[90m\u001B[39;49;00m\n",
      "        read_only = (mode == \u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        prof: Profile = profile \u001B[95mor\u001B[39;49;00m (\u001B[33m\"\u001B[39;49;00m\u001B[33mread\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mwrite\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       db = rs.open(\u001B[90m\u001B[39;49;00m\n",
      "            p,\u001B[90m\u001B[39;49;00m\n",
      "            mode=\u001B[33m\"\u001B[39;49;00m\u001B[33mro\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m read_only \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mrw\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            profile=prof,\u001B[90m\u001B[39;49;00m\n",
      "            create_if_missing=\u001B[95mnot\u001B[39;49;00m read_only,\u001B[90m\u001B[39;49;00m\n",
      "            **open_kwargs,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       RuntimeError: IO error: No such file or directory: While mkdir if missing: /fake/path: No such file or directory\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31msrc/common_db/api.py\u001B[0m:34: RuntimeError\n",
      "\u001B[31m\u001B[1m_____________________________ test_writebatch_path _____________________________\u001B[0m\n",
      "\n",
      "patch_rocks_shim_WriteBatch = <module 'rocks_shim' from '/ext3/miniforge3/envs/hist_w2v/lib/python3.11/site-packages/rocks_shim/__init__.py'>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_writebatch_path\u001B[39;49;00m(patch_rocks_shim_WriteBatch):\u001B[90m\u001B[39;49;00m\n",
      "        db = FakeShimDB_WriteBatch()\u001B[90m\u001B[39;49;00m\n",
      "        data = {\u001B[33m\"\u001B[39;49;00m\u001B[33ma\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m1\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m2\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33mc\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m3\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m}\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       wrote = write_batch_to_db(db, data)\u001B[90m\u001B[39;49;00m\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/db/test_write.py\u001B[0m:91: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "db = <test_write.FakeShimDB_WriteBatch object at 0x14af7c17bf50>\n",
      "pending_data = {'a': b'1', b'b': b'2', 'c': b'3'}\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mwrite_batch_to_db\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "            db: \u001B[33m\"\u001B[39;49;00m\u001B[33mrs.DB\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            pending_data: Mapping[Union[\u001B[96mstr\u001B[39;49;00m, \u001B[96mbytes\u001B[39;49;00m], \u001B[96mbytes\u001B[39;49;00m],\u001B[90m\u001B[39;49;00m\n",
      "    ) -> \u001B[96mint\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "    \u001B[33m    Atomically write entries using rocks-shim batch APIs.\u001B[39;49;00m\n",
      "    \u001B[33m    Uses DB.write_batch() method to create WriteBatch objects.\u001B[39;49;00m\n",
      "    \u001B[33m    \"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m pending_data:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mreturn\u001B[39;49;00m \u001B[94m0\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        n = \u001B[96mlen\u001B[39;49;00m(pending_data)\u001B[90m\u001B[39;49;00m\n",
      "        logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mWriting batch: \u001B[39;49;00m\u001B[33m%s\u001B[39;49;00m\u001B[33m entries\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33mf\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m{\u001B[39;49;00mn\u001B[33m:\u001B[39;49;00m\u001B[33m,\u001B[39;49;00m\u001B[33m}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mtry\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m# Create WriteBatch through the DB object (not rs.WriteBatch())\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      ">           \u001B[94mwith\u001B[39;49;00m db.write_batch() \u001B[94mas\u001B[39;49;00m wb:\u001B[90m\u001B[39;49;00m\n",
      "                 ^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE           AttributeError: 'FakeShimDB_WriteBatch' object has no attribute 'write_batch'\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31msrc/ngram_acquire/db/write.py\u001B[0m:63: AttributeError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.db.write:write.py:72 Error writing batch\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/edk202/ngram-prep/src/ngram_acquire/db/write.py\", line 63, in write_batch_to_db\n",
      "    with db.write_batch() as wb:\n",
      "         ^^^^^^^^^^^^^^\n",
      "AttributeError: 'FakeShimDB_WriteBatch' object has no attribute 'write_batch'\n",
      "\u001B[31m\u001B[1m___________________________ test_context_batch_path ____________________________\u001B[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x14af6d354490>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_context_batch_path\u001B[39;49;00m(monkeypatch):\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Remove WriteBatch so the function must use db.batch()\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mimport\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mrocks_shim\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[94mas\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mrs\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[96mhasattr\u001B[39;49;00m(rs, \u001B[33m\"\u001B[39;49;00m\u001B[33mWriteBatch\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "            monkeypatch.delattr(rs, \u001B[33m\"\u001B[39;49;00m\u001B[33mWriteBatch\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, raising=\u001B[94mFalse\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        db = FakeShimDB_BatchCtx()\u001B[90m\u001B[39;49;00m\n",
      "        data = {\u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mk1\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mv1\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33mk2\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mv2\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m}\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       wrote = write_batch_to_db(db, data)\u001B[90m\u001B[39;49;00m\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/db/test_write.py\u001B[0m:113: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "db = <test_write.FakeShimDB_BatchCtx object at 0x14af6d355590>\n",
      "pending_data = {b'k1': b'v1', 'k2': b'v2'}\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mwrite_batch_to_db\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "            db: \u001B[33m\"\u001B[39;49;00m\u001B[33mrs.DB\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            pending_data: Mapping[Union[\u001B[96mstr\u001B[39;49;00m, \u001B[96mbytes\u001B[39;49;00m], \u001B[96mbytes\u001B[39;49;00m],\u001B[90m\u001B[39;49;00m\n",
      "    ) -> \u001B[96mint\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "    \u001B[33m    Atomically write entries using rocks-shim batch APIs.\u001B[39;49;00m\n",
      "    \u001B[33m    Uses DB.write_batch() method to create WriteBatch objects.\u001B[39;49;00m\n",
      "    \u001B[33m    \"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m pending_data:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mreturn\u001B[39;49;00m \u001B[94m0\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        n = \u001B[96mlen\u001B[39;49;00m(pending_data)\u001B[90m\u001B[39;49;00m\n",
      "        logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mWriting batch: \u001B[39;49;00m\u001B[33m%s\u001B[39;49;00m\u001B[33m entries\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33mf\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m{\u001B[39;49;00mn\u001B[33m:\u001B[39;49;00m\u001B[33m,\u001B[39;49;00m\u001B[33m}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mtry\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m# Create WriteBatch through the DB object (not rs.WriteBatch())\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      ">           \u001B[94mwith\u001B[39;49;00m db.write_batch() \u001B[94mas\u001B[39;49;00m wb:\u001B[90m\u001B[39;49;00m\n",
      "                 ^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE           AttributeError: 'FakeShimDB_BatchCtx' object has no attribute 'write_batch'\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31msrc/ngram_acquire/db/write.py\u001B[0m:63: AttributeError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.db.write:write.py:72 Error writing batch\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/edk202/ngram-prep/src/ngram_acquire/db/write.py\", line 63, in write_batch_to_db\n",
      "    with db.write_batch() as wb:\n",
      "         ^^^^^^^^^^^^^^\n",
      "AttributeError: 'FakeShimDB_BatchCtx' object has no attribute 'write_batch'\n",
      "\u001B[31m\u001B[1m_____________________ test_raises_when_no_batch_supported ______________________\u001B[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x14af6d380290>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_raises_when_no_batch_supported\u001B[39;49;00m(monkeypatch):\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Ensure no WriteBatch present\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mimport\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mrocks_shim\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[94mas\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mrs\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[96mhasattr\u001B[39;49;00m(rs, \u001B[33m\"\u001B[39;49;00m\u001B[33mWriteBatch\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      "            monkeypatch.delattr(rs, \u001B[33m\"\u001B[39;49;00m\u001B[33mWriteBatch\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, raising=\u001B[94mFalse\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        db = FakeShimDB_NoBatch()\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mwith\u001B[39;49;00m pytest.raises(\u001B[96mRuntimeError\u001B[39;49;00m):\u001B[90m\u001B[39;49;00m\n",
      ">           write_batch_to_db(db, {\u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mx\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33my\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m})\u001B[90m\u001B[39;49;00m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/db/test_write.py\u001B[0m:128: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "db = <test_write.FakeShimDB_NoBatch object at 0x14af6d380110>\n",
      "pending_data = {b'x': b'y'}\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mwrite_batch_to_db\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "            db: \u001B[33m\"\u001B[39;49;00m\u001B[33mrs.DB\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            pending_data: Mapping[Union[\u001B[96mstr\u001B[39;49;00m, \u001B[96mbytes\u001B[39;49;00m], \u001B[96mbytes\u001B[39;49;00m],\u001B[90m\u001B[39;49;00m\n",
      "    ) -> \u001B[96mint\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"\u001B[39;49;00m\n",
      "    \u001B[33m    Atomically write entries using rocks-shim batch APIs.\u001B[39;49;00m\n",
      "    \u001B[33m    Uses DB.write_batch() method to create WriteBatch objects.\u001B[39;49;00m\n",
      "    \u001B[33m    \"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m pending_data:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mreturn\u001B[39;49;00m \u001B[94m0\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        n = \u001B[96mlen\u001B[39;49;00m(pending_data)\u001B[90m\u001B[39;49;00m\n",
      "        logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mWriting batch: \u001B[39;49;00m\u001B[33m%s\u001B[39;49;00m\u001B[33m entries\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33mf\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m{\u001B[39;49;00mn\u001B[33m:\u001B[39;49;00m\u001B[33m,\u001B[39;49;00m\u001B[33m}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mtry\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m# Create WriteBatch through the DB object (not rs.WriteBatch())\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      ">           \u001B[94mwith\u001B[39;49;00m db.write_batch() \u001B[94mas\u001B[39;49;00m wb:\u001B[90m\u001B[39;49;00m\n",
      "                 ^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE           AttributeError: 'FakeShimDB_NoBatch' object has no attribute 'write_batch'\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31msrc/ngram_acquire/db/write.py\u001B[0m:63: AttributeError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.db.write:write.py:72 Error writing batch\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/edk202/ngram-prep/src/ngram_acquire/db/write.py\", line 63, in write_batch_to_db\n",
      "    with db.write_batch() as wb:\n",
      "         ^^^^^^^^^^^^^^\n",
      "AttributeError: 'FakeShimDB_NoBatch' object has no attribute 'write_batch'\n",
      "\u001B[31m\u001B[1m________________ test_overwrite_calls_cleanup_and_runs_process _________________\u001B[0m\n",
      "\n",
      "tmp_path = PosixPath('/state/partition1/job-65980021/pytest-of-edk202/pytest-0/test_overwrite_calls_cleanup_a0')\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x14af6d382910>\n",
      "capsys = <_pytest.capture.CaptureFixture object at 0x14af6d3817d0>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_overwrite_calls_cleanup_and_runs_process\u001B[39;49;00m(tmp_path, monkeypatch, capsys):\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Create an existing DB dir to trigger cleanup\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        db_path = tmp_path / \u001B[33m\"\u001B[39;49;00m\u001B[33mdbdir\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        db_path.mkdir()\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        urls = [\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/eng-us-1-00000-of-00002.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/eng-us-1-00001-of-00002.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        ]\u001B[90m\u001B[39;49;00m\n",
      "        calls = _install_stubs(monkeypatch, urls=urls)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       download_and_ingest_to_rocksdb(\u001B[90m\u001B[39;49;00m\n",
      "            ngram_size=\u001B[94m1\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            repo_release_id=\u001B[33m\"\u001B[39;49;00m\u001B[33m20200217\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            repo_corpus_id=\u001B[33m\"\u001B[39;49;00m\u001B[33meng-us-all\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            db_path=\u001B[96mstr\u001B[39;49;00m(db_path),\u001B[90m\u001B[39;49;00m\n",
      "            overwrite=\u001B[94mTrue\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            workers=\u001B[94m2\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            use_threads=\u001B[94mTrue\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            ngram_type=\u001B[33m\"\u001B[39;49;00m\u001B[33mall\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            write_batch_size=\u001B[94m100\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            open_type=\u001B[33m\"\u001B[39;49;00m\u001B[33mdefault\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_orchestrate.py\u001B[0m:114: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "ngram_size = 1, repo_release_id = '20200217', repo_corpus_id = 'eng-us-all'\n",
      "db_path = '/state/partition1/job-65980021/pytest-of-edk202/pytest-0/test_overwrite_calls_cleanup_a0/dbdir'\n",
      "file_range = (0, 1), workers = 2, use_threads = True, ngram_type = 'all'\n",
      "overwrite = True, random_seed = None, write_batch_size = 100\n",
      "open_type = 'default', post_compact = False\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mdownload_and_ingest_to_rocksdb\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "            ngram_size: \u001B[96mint\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            repo_release_id: \u001B[96mstr\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            repo_corpus_id: \u001B[96mstr\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            db_path: \u001B[96mstr\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            file_range: Optional[Tuple[\u001B[96mint\u001B[39;49;00m, \u001B[96mint\u001B[39;49;00m]] = \u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            workers: Optional[\u001B[96mint\u001B[39;49;00m] = \u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            use_threads: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            ngram_type: \u001B[96mstr\u001B[39;49;00m = \u001B[33m\"\u001B[39;49;00m\u001B[33mall\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            overwrite: \u001B[96mbool\u001B[39;49;00m = \u001B[94mTrue\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            random_seed: Optional[\u001B[96mint\u001B[39;49;00m] = \u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            write_batch_size: \u001B[96mint\u001B[39;49;00m = DEFAULT_WRITE_BATCH_SIZE,\u001B[90m\u001B[39;49;00m\n",
      "            open_type: \u001B[96mstr\u001B[39;49;00m = \u001B[33m\"\u001B[39;49;00m\u001B[33mdefault\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,  \u001B[90m# \"default\" | \"write\" | \"read\" | \"bulk_write\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            post_compact: \u001B[96mbool\u001B[39;49;00m = \u001B[94mFalse\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "    ) -> \u001B[94mNone\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m    \u001B[39;49;00m\u001B[33m\"\"\"Discover files, download/parse, and ingest into RocksDB (via rocks-shim).\"\"\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mStarting N-gram processing pipeline\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m _setproctitle \u001B[95mis\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m \u001B[94mNone\u001B[39;49;00m:  \u001B[90m# pragma: no cover\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mtry\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "                _setproctitle.setproctitle(\u001B[33m\"\u001B[39;49;00m\u001B[33mPROC_MAIN\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mexcept\u001B[39;49;00m \u001B[96mException\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[94mpass\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        start_time = datetime.now()\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Worker count\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m workers \u001B[95mis\u001B[39;49;00m \u001B[94mNone\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            cpu = os.cpu_count() \u001B[95mor\u001B[39;49;00m \u001B[94m4\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            workers = \u001B[96mmin\u001B[39;49;00m(\u001B[94m40\u001B[39;49;00m, cpu * \u001B[94m2\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Overwrite existing DB dir\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m overwrite \u001B[95mand\u001B[39;49;00m os.path.exists(db_path):\u001B[90m\u001B[39;49;00m\n",
      "            logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mRemoving existing database for fresh start\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mif\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m safe_db_cleanup(db_path):\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[94mraise\u001B[39;49;00m \u001B[96mRuntimeError\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[33mf\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mFailed to remove existing database at \u001B[39;49;00m\u001B[33m{\u001B[39;49;00mdb_path\u001B[33m}\u001B[39;49;00m\u001B[33m. \u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[33m\"\u001B[39;49;00m\u001B[33mClose open handles or remove it manually.\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "                )\u001B[90m\u001B[39;49;00m\n",
      "            logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mSuccessfully removed existing database\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Ensure parent dir exists\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        db_dir = os.path.dirname(db_path)\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m db_dir:\u001B[90m\u001B[39;49;00m\n",
      "            os.makedirs(db_dir, exist_ok=\u001B[94mTrue\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# 1) Discover files\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        page_url, file_rx = set_location_info(ngram_size, repo_release_id, repo_corpus_id)\u001B[90m\u001B[39;49;00m\n",
      "        file_urls_available = fetch_file_urls(page_url, file_rx)\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m file_urls_available:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mraise\u001B[39;49;00m \u001B[96mRuntimeError\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mNo n-gram files found in the repository\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# 2) Select subset\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m file_range \u001B[95mis\u001B[39;49;00m \u001B[94mNone\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            file_range = (\u001B[94m0\u001B[39;49;00m, \u001B[96mlen\u001B[39;49;00m(file_urls_available) - \u001B[94m1\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        start_idx, end_idx = file_range\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m start_idx < \u001B[94m0\u001B[39;49;00m \u001B[95mor\u001B[39;49;00m end_idx >= \u001B[96mlen\u001B[39;49;00m(file_urls_available) \u001B[95mor\u001B[39;49;00m start_idx > end_idx:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mraise\u001B[39;49;00m \u001B[96mValueError\u001B[39;49;00m(\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[33mf\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mInvalid file range \u001B[39;49;00m\u001B[33m{\u001B[39;49;00mfile_range\u001B[33m}\u001B[39;49;00m\u001B[33m. Available: 0..\u001B[39;49;00m\u001B[33m{\u001B[39;49;00m\u001B[96mlen\u001B[39;49;00m(file_urls_available)\u001B[90m \u001B[39;49;00m-\u001B[90m \u001B[39;49;00m\u001B[94m1\u001B[39;49;00m\u001B[33m}\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            )\u001B[90m\u001B[39;49;00m\n",
      "        file_urls_to_use = file_urls_available[start_idx : end_idx + \u001B[94m1\u001B[39;49;00m]\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# 3) Open DB with rocks-shim\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        ot = (open_type \u001B[95mor\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mdefault\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m).lower()\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m ot \u001B[95mnot\u001B[39;49;00m \u001B[95min\u001B[39;49;00m {\u001B[33m\"\u001B[39;49;00m\u001B[33mdefault\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33mwrite\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33mread\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33mbulk_write\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m}:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mraise\u001B[39;49;00m \u001B[96mValueError\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33mopen_type must be one of \u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33mwrite\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33m, \u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33mread\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33m, \u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33mdefault\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33m, \u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33mbulk_write\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        profile = \u001B[94mNone\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m ot == \u001B[33m\"\u001B[39;49;00m\u001B[33mdefault\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94melse\u001B[39;49;00m ot\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mif\u001B[39;49;00m ot == \u001B[33m\"\u001B[39;49;00m\u001B[33mwrite\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mUsing write-optimized RocksDB options (rocks-shim)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94melif\u001B[39;49;00m ot == \u001B[33m\"\u001B[39;49;00m\u001B[33mread\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mUsing read-optimized RocksDB options (rocks-shim)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94melif\u001B[39;49;00m ot == \u001B[33m\"\u001B[39;49;00m\u001B[33mbulk_write\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mUsing bulk_write-optimized RocksDB options (auto-compaction disabled)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94melse\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "            logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mUsing default RocksDB options (rocks-shim)\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Add this debug code right before \"with open_db(db_path, profile=profile) as db:\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m#print(f\"DEBUG orchestrate.py: open_type='{open_type}', ot='{ot}', profile='{profile}'\")\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mwith\u001B[39;49;00m open_db(db_path, profile=profile) \u001B[94mas\u001B[39;49;00m db:\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m# Add this debug check right after opening\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m#if hasattr(db, 'get_property'):\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m#    try:\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m#        auto_compact = db.get_property(\"rocksdb.disable-auto-compactions\")\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m#        print(f\"DEBUG: After opening, disable_auto_compactions = {auto_compact}\")\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m#    except Exception as e:\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m#        print(f\"DEBUG: Could not read property: {e}\")\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m# Resume filter\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            files_to_skip = \u001B[94m0\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mif\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m overwrite:\u001B[90m\u001B[39;49;00m\n",
      "                to_keep = []\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[94mfor\u001B[39;49;00m url \u001B[95min\u001B[39;49;00m file_urls_to_use:\u001B[90m\u001B[39;49;00m\n",
      "                    name = PurePosixPath(url).name\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mif\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m is_file_processed(db, name):\u001B[90m\u001B[39;49;00m\n",
      "                        to_keep.append(url)\u001B[90m\u001B[39;49;00m\n",
      "                files_to_skip = \u001B[96mlen\u001B[39;49;00m(file_urls_to_use) - \u001B[96mlen\u001B[39;49;00m(to_keep)\u001B[90m\u001B[39;49;00m\n",
      "                file_urls_to_use = to_keep\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[94mif\u001B[39;49;00m files_to_skip:\u001B[90m\u001B[39;49;00m\n",
      "                    logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mResume mode: skipping \u001B[39;49;00m\u001B[33m%s\u001B[39;49;00m\u001B[33m processed files\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, files_to_skip)\u001B[90m\u001B[39;49;00m\n",
      "                \u001B[94mif\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m file_urls_to_use:\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[96mprint\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33m All files in the specified range are already processed!\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "                    \u001B[94mreturn\u001B[39;49;00m  \u001B[90m# context closes DB\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m# Optional shuffle\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mif\u001B[39;49;00m random_seed \u001B[95mis\u001B[39;49;00m \u001B[95mnot\u001B[39;49;00m \u001B[94mNone\u001B[39;49;00m:\u001B[90m\u001B[39;49;00m\n",
      "                random.seed(random_seed)\u001B[90m\u001B[39;49;00m\n",
      "                random.shuffle(file_urls_to_use)\u001B[90m\u001B[39;49;00m\n",
      "                logger.info(\u001B[33m\"\u001B[39;49;00m\u001B[33mRandomized file order with seed \u001B[39;49;00m\u001B[33m%s\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, random_seed)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m# Executor & predicate\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            executor_class: Type = ThreadPoolExecutor \u001B[94mif\u001B[39;49;00m use_threads \u001B[94melse\u001B[39;49;00m ProcessPoolExecutor\u001B[90m\u001B[39;49;00m\n",
      "            executor_name = \u001B[33m\"\u001B[39;49;00m\u001B[33mthreads\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m \u001B[94mif\u001B[39;49;00m use_threads \u001B[94melse\u001B[39;49;00m \u001B[33m\"\u001B[39;49;00m\u001B[33mprocesses\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            filter_pred = make_ngram_type_predicate(ngram_type)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m# 3.5) Summary\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            print_run_summary(\u001B[90m\u001B[39;49;00m\n",
      "                ngram_repo_url=page_url,\u001B[90m\u001B[39;49;00m\n",
      "                db_path=db_path,\u001B[90m\u001B[39;49;00m\n",
      "                file_range=(start_idx, end_idx),\u001B[90m\u001B[39;49;00m\n",
      "                file_urls_available=file_urls_available,\u001B[90m\u001B[39;49;00m\n",
      "                file_urls_to_use=file_urls_to_use,\u001B[90m\u001B[39;49;00m\n",
      "                ngram_size=ngram_size,\u001B[90m\u001B[39;49;00m\n",
      "                workers=workers,\u001B[90m\u001B[39;49;00m\n",
      "                executor_name=executor_name,\u001B[90m\u001B[39;49;00m\n",
      "                start_time=start_time,\u001B[90m\u001B[39;49;00m\n",
      "                ngram_type=ngram_type,\u001B[90m\u001B[39;49;00m\n",
      "                overwrite=overwrite,\u001B[90m\u001B[39;49;00m\n",
      "                files_to_skip=files_to_skip,\u001B[90m\u001B[39;49;00m\n",
      "                write_batch_size=write_batch_size,\u001B[90m\u001B[39;49;00m\n",
      "                color=\u001B[94mTrue\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            )\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "            \u001B[90m# 4) Process & ingest - now returns uncompressed byte count\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      ">           success, failure, written, batches, uncompressed_bytes = process_files(\u001B[90m\u001B[39;49;00m\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "                urls=file_urls_to_use,\u001B[90m\u001B[39;49;00m\n",
      "                executor_class=executor_class,\u001B[90m\u001B[39;49;00m\n",
      "                workers=workers,\u001B[90m\u001B[39;49;00m\n",
      "                db=db,\u001B[90m\u001B[39;49;00m\n",
      "                filter_pred=filter_pred,\u001B[90m\u001B[39;49;00m\n",
      "                write_batch_size=write_batch_size,\u001B[90m\u001B[39;49;00m\n",
      "            )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE           ValueError: not enough values to unpack (expected 5, got 4)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31msrc/ngram_acquire/pipeline/orchestrate.py\u001B[0m:177: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.orchestrate:orchestrate.py:58 Starting N-gram processing pipeline\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.orchestrate:orchestrate.py:75 Removing existing database for fresh start\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.orchestrate:orchestrate.py:81 Successfully removed existing database\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.orchestrate:orchestrate.py:118 Using default RocksDB options (rocks-shim)\n",
      "\u001B[31m\u001B[1m_______________________ test_flush_and_mark_after_write ________________________\u001B[0m\n",
      "\n",
      "install_runner_stubs = (<test_runner.Events object at 0x14af6d38bdd0>, <class 'test_runner.install_runner_stubs.<locals>.CapturingExecutor'>)\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_flush_and_mark_after_write\u001B[39;49;00m(install_runner_stubs):\u001B[90m\u001B[39;49;00m\n",
      "        ev, CapturingExecutor = install_runner_stubs\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        urls = [\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/a.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/b.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        ]\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Each worker returns one entry -> batch size 2 triggers a single flush\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        ev.results_by_url = {\u001B[90m\u001B[39;49;00m\n",
      "            urls[\u001B[94m0\u001B[39;49;00m]: (\u001B[33m\"\u001B[39;49;00m\u001B[33mSUCCESS: a.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, {\u001B[33m\"\u001B[39;49;00m\u001B[33ma\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mx\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m}),\u001B[90m\u001B[39;49;00m\n",
      "            urls[\u001B[94m1\u001B[39;49;00m]: (\u001B[33m\"\u001B[39;49;00m\u001B[33mSUCCESS: b.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, {\u001B[33m\"\u001B[39;49;00m\u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33my\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m}),\u001B[90m\u001B[39;49;00m\n",
      "        }\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       success, failure, written, batches = process_files(\u001B[90m\u001B[39;49;00m\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "            urls=urls,\u001B[90m\u001B[39;49;00m\n",
      "            executor_class=CapturingExecutor,\u001B[90m\u001B[39;49;00m\n",
      "            workers=\u001B[94m3\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            db=\u001B[96mobject\u001B[39;49;00m(),  \u001B[90m# not used by our stubs\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            filter_pred=\u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            write_batch_size=\u001B[94m2\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       ValueError: too many values to unpack (expected 4)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_runner.py\u001B[0m:126: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.runner:runner.py:48 Log file path for workers: /state/partition1/job-65980021/pytest-of-edk202/pytest-0/test_file_location_from_file_p0/ngram_download_20250907_185924.log\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.runner:runner.py:148 ERROR: b.gz - install_runner_stubs.<locals>._worker() takes 3 positional arguments but 4 were given\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.runner:runner.py:148 ERROR: a.gz - install_runner_stubs.<locals>._worker() takes 3 positional arguments but 4 were given\n",
      "\u001B[31m\u001B[1m__________________ test_failure_is_not_marked_and_is_reported __________________\u001B[0m\n",
      "\n",
      "install_runner_stubs = (<test_runner.Events object at 0x14af7c17a950>, <class 'test_runner.install_runner_stubs.<locals>.CapturingExecutor'>)\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_failure_is_not_marked_and_is_reported\u001B[39;49;00m(install_runner_stubs):\u001B[90m\u001B[39;49;00m\n",
      "        ev, CapturingExecutor = install_runner_stubs\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        urls = [\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/ok.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/bad.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        ]\u001B[90m\u001B[39;49;00m\n",
      "        ev.results_by_url = {\u001B[90m\u001B[39;49;00m\n",
      "            urls[\u001B[94m0\u001B[39;49;00m]: (\u001B[33m\"\u001B[39;49;00m\u001B[33mSUCCESS: ok.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, {\u001B[33m\"\u001B[39;49;00m\u001B[33mok\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m1\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m}),\u001B[90m\u001B[39;49;00m\n",
      "            urls[\u001B[94m1\u001B[39;49;00m]: (\u001B[33m\"\u001B[39;49;00m\u001B[33mERROR: bad.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, {}),\u001B[90m\u001B[39;49;00m\n",
      "        }\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       success, failure, written, batches = process_files(\u001B[90m\u001B[39;49;00m\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "            urls=urls,\u001B[90m\u001B[39;49;00m\n",
      "            executor_class=CapturingExecutor,\u001B[90m\u001B[39;49;00m\n",
      "            workers=\u001B[94m1\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            db=\u001B[96mobject\u001B[39;49;00m(),\u001B[90m\u001B[39;49;00m\n",
      "            filter_pred=\u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            write_batch_size=\u001B[94m1\u001B[39;49;00m,  \u001B[90m# flush after the success\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       ValueError: too many values to unpack (expected 4)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_runner.py\u001B[0m:165: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.runner:runner.py:48 Log file path for workers: /state/partition1/job-65980021/pytest-of-edk202/pytest-0/test_file_location_from_file_p0/ngram_download_20250907_185924.log\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.runner:runner.py:148 ERROR: bad.gz - install_runner_stubs.<locals>._worker() takes 3 positional arguments but 4 were given\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.runner:runner.py:148 ERROR: ok.gz - install_runner_stubs.<locals>._worker() takes 3 positional arguments but 4 were given\n",
      "\u001B[31m\u001B[1m____________ test_db_write_error_records_failures_and_does_not_mark ____________\u001B[0m\n",
      "\n",
      "install_runner_stubs = (<test_runner.Events object at 0x14af7c11c290>, <class 'test_runner.install_runner_stubs.<locals>.CapturingExecutor'>)\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_db_write_error_records_failures_and_does_not_mark\u001B[39;49;00m(install_runner_stubs):\u001B[90m\u001B[39;49;00m\n",
      "        ev, CapturingExecutor = install_runner_stubs\u001B[90m\u001B[39;49;00m\n",
      "        ev.fail_db_write = \u001B[94mTrue\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        urls = [\u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/e.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m]\u001B[90m\u001B[39;49;00m\n",
      "        ev.results_by_url = {\u001B[90m\u001B[39;49;00m\n",
      "            urls[\u001B[94m0\u001B[39;49;00m]: (\u001B[33m\"\u001B[39;49;00m\u001B[33mSUCCESS: e.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, {\u001B[33m\"\u001B[39;49;00m\u001B[33me\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m!!\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m}),\u001B[90m\u001B[39;49;00m\n",
      "        }\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       success, failure, written, batches = process_files(\u001B[90m\u001B[39;49;00m\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "            urls=urls,\u001B[90m\u001B[39;49;00m\n",
      "            executor_class=CapturingExecutor,\u001B[90m\u001B[39;49;00m\n",
      "            workers=\u001B[94m1\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            db=\u001B[96mobject\u001B[39;49;00m(),\u001B[90m\u001B[39;49;00m\n",
      "            filter_pred=\u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            write_batch_size=\u001B[94m1\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       ValueError: too many values to unpack (expected 4)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_runner.py\u001B[0m:195: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.runner:runner.py:48 Log file path for workers: /state/partition1/job-65980021/pytest-of-edk202/pytest-0/test_file_location_from_file_p0/ngram_download_20250907_185924.log\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.runner:runner.py:148 ERROR: e.gz - install_runner_stubs.<locals>._worker() takes 3 positional arguments but 4 were given\n",
      "\u001B[31m\u001B[1m_________________ test_final_flush_when_threshold_not_reached __________________\u001B[0m\n",
      "\n",
      "install_runner_stubs = (<test_runner.Events object at 0x14af7c0f0590>, <class 'test_runner.install_runner_stubs.<locals>.CapturingExecutor'>)\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_final_flush_when_threshold_not_reached\u001B[39;49;00m(install_runner_stubs):\u001B[90m\u001B[39;49;00m\n",
      "        ev, CapturingExecutor = install_runner_stubs\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        urls = [\u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/u.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/v.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m]\u001B[90m\u001B[39;49;00m\n",
      "        ev.results_by_url = {\u001B[90m\u001B[39;49;00m\n",
      "            urls[\u001B[94m0\u001B[39;49;00m]: (\u001B[33m\"\u001B[39;49;00m\u001B[33mSUCCESS: u.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, {\u001B[33m\"\u001B[39;49;00m\u001B[33mu\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mx\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m}),\u001B[90m\u001B[39;49;00m\n",
      "            urls[\u001B[94m1\u001B[39;49;00m]: (\u001B[33m\"\u001B[39;49;00m\u001B[33mSUCCESS: v.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, {\u001B[33m\"\u001B[39;49;00m\u001B[33mv\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33my\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m}),\u001B[90m\u001B[39;49;00m\n",
      "        }\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Threshold too high -> no mid-run flush; final flush at the end\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      ">       success, failure, written, batches = process_files(\u001B[90m\u001B[39;49;00m\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "            urls=urls,\u001B[90m\u001B[39;49;00m\n",
      "            executor_class=CapturingExecutor,\u001B[90m\u001B[39;49;00m\n",
      "            workers=\u001B[94m2\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            db=\u001B[96mobject\u001B[39;49;00m(),\u001B[90m\u001B[39;49;00m\n",
      "            filter_pred=\u001B[94mNone\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            write_batch_size=\u001B[94m10\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       ValueError: too many values to unpack (expected 4)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_runner.py\u001B[0m:221: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.runner:runner.py:48 Log file path for workers: /state/partition1/job-65980021/pytest-of-edk202/pytest-0/test_file_location_from_file_p0/ngram_download_20250907_185924.log\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.runner:runner.py:148 ERROR: u.gz - install_runner_stubs.<locals>._worker() takes 3 positional arguments but 4 were given\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.runner:runner.py:148 ERROR: v.gz - install_runner_stubs.<locals>._worker() takes 3 positional arguments but 4 were given\n",
      "\u001B[31m\u001B[1m________________________ test_success_parses_and_packs _________________________\u001B[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x14af7c15c150>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_success_parses_and_packs\u001B[39;49;00m(monkeypatch):\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# 3 lines: 2 tagged (kept), 1 untagged (filtered)\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        lines = [\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33malpha_NOUN\u001B[39;49;00m\u001B[33m\\t\u001B[39;49;00m\u001B[33m2000,10,2\u001B[39;49;00m\u001B[33m\\t\u001B[39;49;00m\u001B[33m2001,20,4\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mbeta_VERB\u001B[39;49;00m\u001B[33m\\t\u001B[39;49;00m\u001B[33m1999,3,5\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mgamma delta\u001B[39;49;00m\u001B[33m\\t\u001B[39;49;00m\u001B[33m2001,1,1\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        ]\u001B[90m\u001B[39;49;00m\n",
      "        resp = FakeResponse(_gz_bytes(lines), headers={\u001B[33m\"\u001B[39;49;00m\u001B[33mcontent-length\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33m\"\u001B[39;49;00m\u001B[33m1234\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m})\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[90m# Patch download to return our fake streaming response\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mimport\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mngram_acquire\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mpipeline\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mworker\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[94mas\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mworker_mod\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        monkeypatch.setattr(worker_mod, \u001B[33m\"\u001B[39;49;00m\u001B[33mstream_download_with_retries\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[94mlambda\u001B[39;49;00m *a, **k: resp)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        pred = make_ngram_type_predicate(\u001B[33m\"\u001B[39;49;00m\u001B[33mtagged\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       msg, data = process_and_ingest_file(\u001B[90m\u001B[39;49;00m\n",
      "        ^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://example.com/eng-us-1-00000-of-00001.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            worker_id=\u001B[94m1\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            filter_pred=pred,\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       ValueError: too many values to unpack (expected 2)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_worker.py\u001B[0m:54: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.worker:worker.py:85 Worker 1 (PID 2973170): Processing eng-us-1-00000-of-00001.gz\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.worker:worker.py:92 Worker 1 (PID 2973170): File size: 1,234 bytes (compressed)\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.worker:worker.py:143 Worker 1 (PID 2973170): SUCCESS: eng-us-1-00000-of-00001.gz - 3 lines, 2 entries, 70 uncompressed bytes\n",
      "\u001B[31m\u001B[1m_________ test_unicode_decode_error_is_logged_and_processing_continues _________\u001B[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x14af6d357690>\n",
      "caplog = <_pytest.logging.LogCaptureFixture object at 0x14af6d355510>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_unicode_decode_error_is_logged_and_processing_continues\u001B[39;49;00m(monkeypatch, caplog):\u001B[90m\u001B[39;49;00m\n",
      "        lines = [\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33mx_NOUN\u001B[39;49;00m\u001B[33m\\t\u001B[39;49;00m\u001B[33m1990,1,1\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33m\\xff\u001B[39;49;00m\u001B[33m\\xfe\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,  \u001B[90m# invalid UTF-8\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33mb\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m\u001B[33my_VERB\u001B[39;49;00m\u001B[33m\\t\u001B[39;49;00m\u001B[33m1991,2,2\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m,\u001B[90m\u001B[39;49;00m\n",
      "        ]\u001B[90m\u001B[39;49;00m\n",
      "        resp = FakeResponse(_gz_bytes(lines), headers={\u001B[33m\"\u001B[39;49;00m\u001B[33mcontent-length\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m: \u001B[33m\"\u001B[39;49;00m\u001B[33mweird\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m})\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mimport\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mngram_acquire\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mpipeline\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mworker\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[94mas\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mworker_mod\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "        monkeypatch.setattr(worker_mod, \u001B[33m\"\u001B[39;49;00m\u001B[33mstream_download_with_retries\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, \u001B[94mlambda\u001B[39;49;00m *a, **k: resp)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        pred = make_ngram_type_predicate(\u001B[33m\"\u001B[39;49;00m\u001B[33mtagged\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        caplog.set_level(\u001B[33m\"\u001B[39;49;00m\u001B[33mWARNING\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       msg, data = process_and_ingest_file(\u001B[90m\u001B[39;49;00m\n",
      "        ^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/file.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, worker_id=\u001B[94m2\u001B[39;49;00m, filter_pred=pred\u001B[90m\u001B[39;49;00m\n",
      "        )\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       ValueError: too many values to unpack (expected 2)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_worker.py\u001B[0m:92: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[33mWARNING \u001B[0m ngram_acquire.pipeline.worker:worker.py:120 Worker 2 (PID 2973170): Unicode error in file.gz line 2: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\u001B[31m\u001B[1m_________________ test_timeout_returns_message_and_empty_dict __________________\u001B[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x14af6d383a50>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_timeout_returns_message_and_empty_dict\u001B[39;49;00m(monkeypatch):\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mimport\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mngram_acquire\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mpipeline\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mworker\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[94mas\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mworker_mod\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mboom\u001B[39;49;00m(*a, **k):\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mraise\u001B[39;49;00m requests.Timeout(\u001B[33m\"\u001B[39;49;00m\u001B[33mslow\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        monkeypatch.setattr(worker_mod, \u001B[33m\"\u001B[39;49;00m\u001B[33mstream_download_with_retries\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, boom)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       msg, data = process_and_ingest_file(\u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/file.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, worker_id=\u001B[94m3\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        ^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       ValueError: too many values to unpack (expected 2)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_worker.py\u001B[0m:111: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.worker:worker.py:85 Worker 3 (PID 2973170): Processing file.gz\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.worker:worker.py:148 Worker 3 (PID 2973170): Timeout - file.gz\n",
      "\u001B[31m\u001B[1m______________ test_network_error_returns_message_and_empty_dict _______________\u001B[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x14af7c198990>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_network_error_returns_message_and_empty_dict\u001B[39;49;00m(monkeypatch):\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mimport\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mngram_acquire\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mpipeline\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mworker\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[94mas\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mworker_mod\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mboom\u001B[39;49;00m(*a, **k):\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mraise\u001B[39;49;00m requests.RequestException(\u001B[33m\"\u001B[39;49;00m\u001B[33mdown\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        monkeypatch.setattr(worker_mod, \u001B[33m\"\u001B[39;49;00m\u001B[33mstream_download_with_retries\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, boom)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       msg, data = process_and_ingest_file(\u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/file.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, worker_id=\u001B[94m4\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        ^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       ValueError: too many values to unpack (expected 2)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_worker.py\u001B[0m:124: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.worker:worker.py:85 Worker 4 (PID 2973170): Processing file.gz\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.worker:worker.py:152 Worker 4 (PID 2973170): Network error - file.gz (down)\n",
      "\u001B[31m\u001B[1m____________ test_generic_exception_returns_message_and_empty_dict _____________\u001B[0m\n",
      "\n",
      "monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x14af6d355b10>\n",
      "\n",
      "    \u001B[0m\u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mtest_generic_exception_returns_message_and_empty_dict\u001B[39;49;00m(monkeypatch):\u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mimport\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mngram_acquire\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mpipeline\u001B[39;49;00m\u001B[04m\u001B[96m.\u001B[39;49;00m\u001B[04m\u001B[96mworker\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[94mas\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[04m\u001B[96mworker_mod\u001B[39;49;00m\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        \u001B[94mdef\u001B[39;49;00m\u001B[90m \u001B[39;49;00m\u001B[92mboom\u001B[39;49;00m(*a, **k):\u001B[90m\u001B[39;49;00m\n",
      "            \u001B[94mraise\u001B[39;49;00m \u001B[96mRuntimeError\u001B[39;49;00m(\u001B[33m\"\u001B[39;49;00m\u001B[33moops\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      "        monkeypatch.setattr(worker_mod, \u001B[33m\"\u001B[39;49;00m\u001B[33mstream_download_with_retries\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, boom)\u001B[90m\u001B[39;49;00m\n",
      "    \u001B[90m\u001B[39;49;00m\n",
      ">       msg, data = process_and_ingest_file(\u001B[33m\"\u001B[39;49;00m\u001B[33mhttps://ex/file.gz\u001B[39;49;00m\u001B[33m\"\u001B[39;49;00m, worker_id=\u001B[94m5\u001B[39;49;00m)\u001B[90m\u001B[39;49;00m\n",
      "        ^^^^^^^^^\u001B[90m\u001B[39;49;00m\n",
      "\u001B[1m\u001B[31mE       ValueError: too many values to unpack (expected 2)\u001B[0m\n",
      "\n",
      "\u001B[1m\u001B[31mtests/ngram_acquire/pipeline/test_worker.py\u001B[0m:137: ValueError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "\u001B[32mINFO    \u001B[0m ngram_acquire.pipeline.worker:worker.py:85 Worker 5 (PID 2973170): Processing file.gz\n",
      "\u001B[1m\u001B[31mERROR   \u001B[0m ngram_acquire.pipeline.worker:worker.py:158 Worker 5 (PID 2973170): Error - file.gz: oops\n",
      "\u001B[36m\u001B[1m=========================== short test summary info ============================\u001B[0m\n",
      "\u001B[31mFAILED\u001B[0m tests/common_db/test_api.py::\u001B[1mTestOpenDb::test_open_db_bulk_finalization\u001B[0m - TypeError: open(): incompatible function arguments. The following argument ...\n",
      "\u001B[31mFAILED\u001B[0m tests/common_db/test_api.py::\u001B[1mTestOpenDb::test_open_db_exception_handling\u001B[0m - RuntimeError: IO error: No such file or directory: While mkdir if missing: ...\n",
      "\u001B[31mFAILED\u001B[0m tests/common_db/test_api.py::\u001B[1mTestOpenDb::test_open_db_no_finalization_methods\u001B[0m - RuntimeError: IO error: No such file or directory: While mkdir if missing: ...\n",
      "\u001B[31mFAILED\u001B[0m tests/common_db/test_api.py::\u001B[1mtest_open_invocation_shape_read_only\u001B[0m - RuntimeError: IO error: No such file or directory: While opening a file for...\n",
      "\u001B[31mFAILED\u001B[0m tests/common_db/test_api.py::\u001B[1mtest_open_invocation_shape_read_write\u001B[0m - RuntimeError: IO error: No such file or directory: While mkdir if missing: ...\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/db/test_write.py::\u001B[1mtest_writebatch_path\u001B[0m - AttributeError: 'FakeShimDB_WriteBatch' object has no attribute 'write_batch'\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/db/test_write.py::\u001B[1mtest_context_batch_path\u001B[0m - AttributeError: 'FakeShimDB_BatchCtx' object has no attribute 'write_batch'\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/db/test_write.py::\u001B[1mtest_raises_when_no_batch_supported\u001B[0m - AttributeError: 'FakeShimDB_NoBatch' object has no attribute 'write_batch'\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_orchestrate.py::\u001B[1mtest_overwrite_calls_cleanup_and_runs_process\u001B[0m - ValueError: not enough values to unpack (expected 5, got 4)\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_runner.py::\u001B[1mtest_flush_and_mark_after_write\u001B[0m - ValueError: too many values to unpack (expected 4)\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_runner.py::\u001B[1mtest_failure_is_not_marked_and_is_reported\u001B[0m - ValueError: too many values to unpack (expected 4)\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_runner.py::\u001B[1mtest_db_write_error_records_failures_and_does_not_mark\u001B[0m - ValueError: too many values to unpack (expected 4)\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_runner.py::\u001B[1mtest_final_flush_when_threshold_not_reached\u001B[0m - ValueError: too many values to unpack (expected 4)\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_worker.py::\u001B[1mtest_success_parses_and_packs\u001B[0m - ValueError: too many values to unpack (expected 2)\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_worker.py::\u001B[1mtest_unicode_decode_error_is_logged_and_processing_continues\u001B[0m - ValueError: too many values to unpack (expected 2)\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_worker.py::\u001B[1mtest_timeout_returns_message_and_empty_dict\u001B[0m - ValueError: too many values to unpack (expected 2)\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_worker.py::\u001B[1mtest_network_error_returns_message_and_empty_dict\u001B[0m - ValueError: too many values to unpack (expected 2)\n",
      "\u001B[31mFAILED\u001B[0m tests/ngram_acquire/pipeline/test_worker.py::\u001B[1mtest_generic_exception_returns_message_and_empty_dict\u001B[0m - ValueError: too many values to unpack (expected 2)\n",
      "\u001B[31m\u001B[31m\u001B[1m18 failed\u001B[0m, \u001B[32m81 passed\u001B[0m\u001B[31m in 1.94s\u001B[0m\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.TESTS_FAILED: 1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T06:58:10.961077Z",
     "start_time": "2025-09-06T06:58:10.237878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pytest.main([\n",
    "    \"-q\",\n",
    "    \"/scratch/edk202/ngram-prep/tests/ngram_acquire/db/test_write_shim_variants.py\"\n",
    "])"
   ],
   "id": "1048080fa5a2fd6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m.\u001B[0m\u001B[32m.\u001B[0m\u001B[32m                                                                       [100%]\u001B[0m\n",
      "\u001B[32m\u001B[32m\u001B[1m2 passed\u001B[0m\u001B[32m in 0.01s\u001B[0m\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7141a6ebfb8e09fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist_w2v (Singularity)",
   "language": "python",
   "name": "hist_w2v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
