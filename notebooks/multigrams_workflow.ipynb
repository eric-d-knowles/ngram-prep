{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ],
   "id": "b76a1e3e9b344be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "98dd936c0a392494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:50:16.142279Z",
     "start_time": "2025-11-08T17:50:16.115592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from stop_words import get_stop_words\n",
    "from ngram_prep.ngram_filter.lemmatizer import SpacyLemmatizer\n",
    "from ngram_prep.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngram_prep.ngram_filter.config import PipelineConfig as FilterPipelineConfig\n",
    "from ngram_prep.ngram_filter.config import FilterConfig\n",
    "from ngram_prep.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngram_prep.ngram_pivot.config import PipelineConfig as PivotPipelineConfig\n",
    "from ngram_prep.ngram_pivot.pipeline import run_pivot_pipeline\n",
    "from ngram_prep.utilities.peek import db_head, db_peek, db_peek_prefix\n",
    "from ngram_prep.utilities.notebook_logging import setup_notebook_logging"
   ],
   "id": "f261bd4a6317873c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": "### Configure Paths"
  },
  {
   "cell_type": "code",
   "id": "config_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:50:17.235207Z",
     "start_time": "2025-11-08T17:50:17.223171Z"
    }
   },
   "source": [
    "base_path = Path(\"/scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files\")\n",
    "raw_db = base_path / \"5grams.db\"\n",
    "filtered_db = base_path / \"5grams_processed.db\"\n",
    "pivoted_db = base_path / \"5grams_pivoted.db\"\n",
    "filter_tmp_dir = base_path / \"processing_tmp\"\n",
    "pivot_tmp_dir = base_path / \"pivot_tmp\"\n",
    "whitelist_path = \"/scratch/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\""
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": "## **Step 1: Download and Ingest**"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T07:39:44.031144Z",
     "start_time": "2025-11-08T02:50:15.226426Z"
    }
   },
   "source": [
    "setup_notebook_logging(\n",
    "    workflow_name=\"multigrams_acquire\",\n",
    "    console=False\n",
    ")\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=5,\n",
    "    repo_release_id=\"20200217\",\n",
    "    repo_corpus_id=\"eng\",\n",
    "    db_path_stub=\"/scratch/edk202/NLP_corpora/Google_Books/\",\n",
    "    file_range=(0, 19422),\n",
    "    random_seed=76,\n",
    "    workers=100,\n",
    "    use_threads=False,\n",
    "    ngram_type=\"tagged\",\n",
    "    overwrite_db=False,\n",
    "    write_batch_size=1_000_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-07 21:50:15\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng/5-\n",
      "DB path:              /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "File range:           0 to 19422\n",
      "Total files:          19423\n",
      "Files to get:         19423\n",
      "Skipping:             0\n",
      "Download workers:     100\n",
      "Batch size:           1,000,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         False\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|█████████████████████████████████████████████████| 19423/19423 [3:24:46<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Ingestion Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         2.20 TB\n",
      "Compaction completed in 1:24:29\n",
      "Size before:             2.20 TB\n",
      "Size after:              2.21 TB\n",
      "Space saved:             -7.89 GB (-0.3%)\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       19422\n",
      "Failed files:                1\n",
      "Total entries written:       2,478,900,513\n",
      "Write batches flushed:       5670\n",
      "Uncompressed data processed: 28.44 TB\n",
      "Processing throughput:       1716.69 MB/sec\n",
      "\n",
      "End Time: 2025-11-08 02:39:43.914337\n",
      "Total Runtime: 4:49:28.622772\n",
      "Time per file: 0:00:00.894276\n",
      "Files per hour: 4025.6\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing `FilterConfig` and `FilterPipelineConfig` objects to the `build_processed_db` function. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did in the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "id": "phase2_run",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-08T17:50:26.992427Z"
    }
   },
   "source": [
    "setup_notebook_logging(\n",
    "    workflow_name=\"multigrams_filter\",\n",
    "    data_path=str(base_path),\n",
    "    console=False\n",
    ")\n",
    "\n",
    "stop_set = set(get_stop_words(\"english\"))\n",
    "lemmatizer = SpacyLemmatizer(language=\"en\")\n",
    "\n",
    "filter_config = FilterConfig(\n",
    "    stop_set=stop_set,\n",
    "    lemma_gen=lemmatizer,\n",
    "    whitelist_path=whitelist_path\n",
    ")\n",
    "\n",
    "pipeline_config = FilterPipelineConfig(\n",
    "    src_db=raw_db,\n",
    "    dst_db=filtered_db,\n",
    "    tmp_dir=filter_tmp_dir,\n",
    "    num_workers=80,\n",
    "    use_smart_partitioning=True,\n",
    "    num_initial_work_units=1000,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    samples_per_worker=500_000,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    flush_interval_s=5.0,\n",
    "    mode=\"reprocess\",\n",
    "    progress_every_s=30.0,\n",
    "    ingest_num_readers=10,\n",
    "    ingest_batch_items=1_000_000,\n",
    "    ingest_queue_size=3,\n",
    "    compact_after_ingest=False\n",
    ")\n",
    "\n",
    "build_processed_db(pipeline_config, filter_config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\u001B[4mPipeline\u001B[0m\n",
      "Run mode: reprocess\n",
      "\n",
      "\u001B[4mWorkers\u001B[0m\n",
      "Num Workers:        80\n",
      "Initial work units: 1000\n",
      "Profiles:           read=read:packed24, write=write:packed24\n",
      "Flush interval:     5.0s\n",
      "\n",
      "\u001B[4mFiles\u001B[0m\n",
      "Source: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "Destination: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Input whitelist: ..._corpora/Google_Books/20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist: None\n",
      "Loading whitelist...\n",
      "Loaded 6,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Reprocess - using cached partitions if available\n",
      "Found existing work units - resetting their status\n",
      "\n",
      "Phase 2: Processing 699 work units with 80 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": "## **Step 3: Pivot to Yearly Indices**"
  },
  {
   "cell_type": "code",
   "id": "phase3_run",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-04T20:16:56.363756Z"
    }
   },
   "source": [
    "setup_notebook_logging(\n",
    "    workflow_name=\"multigrams_pivot\",\n",
    "    data_path=str(base_path),\n",
    "    console=False\n",
    ")\n",
    "\n",
    "pivot_config = PivotPipelineConfig(\n",
    "    src_db=filtered_db,\n",
    "    dst_db=pivoted_db,\n",
    "    tmp_dir=pivot_tmp_dir,\n",
    "    num_workers=30,\n",
    "    num_initial_work_units=1000,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    samples_per_worker=500_000,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    flush_interval_s=15.0,\n",
    "    progress_every_s=30.0,\n",
    "    mode=\"restart\",\n",
    "    num_ingest_readers=1,\n",
    "    ingest_buffer_shards=1,\n",
    "    use_smart_partitioning=True,\n",
    "    ingest_mode=\"direct_sst\"\n",
    ")\n",
    "\n",
    "run_pivot_pipeline(pivot_config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-04 15:16:56\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Target DB:            .../edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/pivot_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              30\n",
      "Initial work units:   1000\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Flush Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       15.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Sampling database to create 1000 density-based work units...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T23:31:08.509799Z",
     "start_time": "2025-11-04T23:31:07.472671Z"
    }
   },
   "source": [
    "db_head(str(pivoted_db), n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1470] <UNK> <UNK> <UNK> <UNK> eng\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 2] Key:   [1470] <UNK> <UNK> <UNK> atomic energy\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1470] <UNK> <UNK> <UNK> eng <UNK>\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1470] <UNK> <UNK> <UNK> much convenient\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1470] <UNK> <UNK> <UNK> selection <UNK>\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_peek",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T23:31:13.403497Z",
     "start_time": "2025-11-04T23:31:12.911176Z"
    }
   },
   "source": [
    "db_peek(str(pivoted_db), start_key=\"[2000] quick\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007d0717569636b:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2000] quick <UNK> <UNK> <UNK> <UNK>\n",
      "     Value: 8,774 occurrences in 8,333 documents\n",
      "\n",
      "[ 2] Key:   [2000] quick <UNK> <UNK> <UNK> ability\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [2000] quick <UNK> <UNK> <UNK> able\n",
      "     Value: 3 occurrences in 3 documents\n",
      "\n",
      "[ 4] Key:   [2000] quick <UNK> <UNK> <UNK> accidental\n",
      "     Value: 4 occurrences in 4 documents\n",
      "\n",
      "[ 5] Key:   [2000] quick <UNK> <UNK> <UNK> accurate\n",
      "     Value: 8 occurrences in 8 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "inspect_prefix_header",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T23:31:16.449019Z",
     "start_time": "2025-11-04T23:31:15.951005Z"
    }
   },
   "source": [
    "db_peek_prefix(str(pivoted_db), prefix=\"[2011] unite\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007db756e697465:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2011] unite <UNK> <UNK> <UNK> <UNK>\n",
      "     Value: 24,615 occurrences in 19,900 documents\n",
      "\n",
      "[ 2] Key:   [2011] unite <UNK> <UNK> <UNK> able\n",
      "     Value: 12 occurrences in 12 documents\n",
      "\n",
      "[ 3] Key:   [2011] unite <UNK> <UNK> <UNK> accomplish\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 4] Key:   [2011] unite <UNK> <UNK> <UNK> accomplishment\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [2011] unite <UNK> <UNK> <UNK> accord\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24a850b70859d225"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
