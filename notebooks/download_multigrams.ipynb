{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T02:30:15.500185Z",
     "start_time": "2025-09-22T02:30:01.408010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%cd /scratch/edk202/ngram-prep\n",
    "\n",
    "%env LC_ALL=C.UTF-8\n",
    "%env LANG=C.UTF-8\n",
    "\n",
    "%pip install -e . --no-build-isolation -q"
   ],
   "id": "e2b29f8010b30c2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/edk202/ngram-prep\n",
      "env: LC_ALL=C.UTF-8\n",
      "env: LANG=C.UTF-8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T02:30:17.225537Z",
     "start_time": "2025-09-22T02:30:17.185378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Auto-reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "ce854c30d3ed2553",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T02:30:17.819495Z",
     "start_time": "2025-09-22T02:30:17.780592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Standard stuff\n",
    "from pathlib import Path\n",
    "\n",
    "# NLTK stuff\n",
    "from nltk.corpus import stopwords; stopwords = set(stopwords.words(\"english\"))\n",
    "from nltk.stem import WordNetLemmatizer; lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Raw n-gram acquisition stuff\n",
    "from ngram_acquire.pipeline.orchestrate import download_and_ingest_to_rocksdb\n",
    "from ngram_acquire.pipeline.logger import setup_logger\n",
    "\n",
    "# Cython utilities\n",
    "from ngram_filter.config import PipelineConfig, FilterConfig\n",
    "from ngram_filter.pipeline.orchestrator import build_processed_db"
   ],
   "id": "66bfa7710298d111",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T02:30:19.223454Z",
     "start_time": "2025-09-22T02:30:19.160506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "setup_logger(\n",
    "    db_path=\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams.db\",\n",
    "    console=False,\n",
    "    rotate=True,\n",
    "    max_bytes=100_000_000,\n",
    "    backup_count=5,\n",
    "    force=True\n",
    ")"
   ],
   "id": "fd41f453d3936ce3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams.db/ngram_download_20250921_223019.log')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Download 5-Grams and Ingest to a RocksDB Database**",
   "id": "51b58f57597b05ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T12:57:14.902235Z",
     "start_time": "2025-09-19T12:57:09.699496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size = 5,\n",
    "    repo_release_id = \"20200217\",\n",
    "    repo_corpus_id = \"eng\",\n",
    "    db_path = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\",\n",
    "    file_range = (0, 19422),\n",
    "    random_seed = 11,\n",
    "    workers = 40,\n",
    "    use_threads = False,\n",
    "    ngram_type = \"tagged\",\n",
    "    overwrite = False,\n",
    "    write_batch_size = 100_000,\n",
    "    open_type = \"write:packed24\",\n",
    "    post_compact = True\n",
    ")"
   ],
   "id": "65902ca87dcac5b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ All files in the specified range are already processed!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T00:59:19.973810Z",
     "start_time": "2025-09-18T23:07:26.725713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from common_db.api import open_db\n",
    "\n",
    "db_path = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\"\n",
    "\n",
    "with open_db(db_path, mode=\"rw\") as db:\n",
    "    db.compact_all()"
   ],
   "id": "40931c497e1c4aea",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Process the Ngrams",
   "id": "7b87630cc83eaebb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T02:24:07.714301Z",
     "start_time": "2025-09-21T23:25:14.820702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_db = Path(\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\")\n",
    "dst_db = src_db.parent / \"5grams_processed.db\"\n",
    "tmp_dir = src_db.parent / \"processing_tmp\"\n",
    "wht_path =\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1grams_processed.db/whitelist.txt\"\n",
    "\n",
    "pipeline_config = PipelineConfig(\n",
    "    src_db=src_db,\n",
    "    dst_db=dst_db,\n",
    "    tmp_dir=tmp_dir,\n",
    "    readers=16,\n",
    "    partitioning_sample_rate=0.0001,\n",
    "    prefix_length=4,\n",
    "    force_restart=True,\n",
    "    progress_every_s=60.0,\n",
    ")\n",
    "\n",
    "filter_config = FilterConfig(\n",
    "    stop_set=stopwords,\n",
    "    lemma_gen=lemmatizer,\n",
    "    whitelist_path=wht_path\n",
    ")\n",
    "\n",
    "build_processed_db(pipeline_config, filter_config)"
   ],
   "id": "e599bcdf6c73be2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM FILTER PIPELINE\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "Configuration:\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  Workers: 16\n",
      "  Work units: 128\n",
      "  Source: /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "  Destination: 02/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "  Buffer: 25,000 items, 16MB\n",
      "  Profile: write:packed24\n",
      "  Input whitelist: .../Google_Books/20200217/eng/5gram_files/1grams_processed.db/whitelist.txt\n",
      "    All tokens (min count: 1)\n",
      "  Output whitelist: None\n",
      "  Loading whitelist...\n",
      "  Loaded 40,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "  Force restart requested - clearing existing work units\n",
      "  Sampling database at 0.0001 rate (prefix_length=4)...\n",
      "  Targeting 248,014 samples using reservoir sampling\n",
      "  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "  RESERVOIR SAMPLING CONFIGURATION\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Target sample size:     248,014 items\n",
      "  Progress interval:      10,000,000 items\n",
      "  Database limit:         No limit (full traversal)\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Processed 10,000,000 items\n",
      "  Processed 20,000,000 items\n",
      "  Processed 30,000,000 items\n",
      "  Processed 40,000,000 items\n",
      "  Processed 50,000,000 items\n",
      "  Processed 60,000,000 items\n",
      "  Processed 70,000,000 items\n",
      "  Processed 80,000,000 items\n",
      "  Processed 90,000,000 items\n",
      "  Processed 100,000,000 items\n",
      "  Processed 110,000,000 items\n",
      "  Processed 120,000,000 items\n",
      "  Processed 130,000,000 items\n",
      "  Processed 140,000,000 items\n",
      "  Processed 150,000,000 items\n",
      "  Processed 160,000,000 items\n",
      "  Processed 170,000,000 items\n",
      "  Processed 180,000,000 items\n",
      "  Processed 190,000,000 items\n",
      "  Processed 200,000,000 items\n",
      "  Processed 210,000,000 items\n",
      "  Processed 220,000,000 items\n",
      "  Processed 230,000,000 items\n",
      "  Processed 240,000,000 items\n",
      "  Processed 250,000,000 items\n",
      "  Processed 260,000,000 items\n",
      "  Processed 270,000,000 items\n",
      "  Processed 280,000,000 items\n",
      "  Processed 290,000,000 items\n",
      "  Processed 300,000,000 items\n",
      "  Processed 310,000,000 items\n",
      "  Processed 320,000,000 items\n",
      "  Processed 330,000,000 items\n",
      "  Processed 340,000,000 items\n",
      "  Processed 350,000,000 items\n",
      "  Processed 360,000,000 items\n",
      "  Processed 370,000,000 items\n",
      "  Processed 380,000,000 items\n",
      "  Processed 390,000,000 items\n",
      "  Processed 400,000,000 items\n",
      "  Processed 410,000,000 items\n",
      "  Processed 420,000,000 items\n",
      "  Processed 430,000,000 items\n",
      "  Processed 440,000,000 items\n",
      "  Processed 450,000,000 items\n",
      "  Processed 460,000,000 items\n",
      "  Processed 470,000,000 items\n",
      "  Processed 480,000,000 items\n",
      "  Processed 490,000,000 items\n",
      "  Processed 500,000,000 items\n",
      "  Processed 510,000,000 items\n",
      "  Processed 520,000,000 items\n",
      "  Processed 530,000,000 items\n",
      "  Processed 540,000,000 items\n",
      "  Processed 550,000,000 items\n",
      "  Processed 560,000,000 items\n",
      "  Processed 570,000,000 items\n",
      "  Processed 580,000,000 items\n",
      "  Processed 590,000,000 items\n",
      "  Processed 600,000,000 items\n",
      "  Processed 610,000,000 items\n",
      "  Processed 620,000,000 items\n",
      "  Processed 630,000,000 items\n",
      "  Processed 640,000,000 items\n",
      "  Processed 650,000,000 items\n",
      "  Processed 660,000,000 items\n",
      "  Processed 670,000,000 items\n",
      "  Processed 680,000,000 items\n",
      "  Processed 690,000,000 items\n",
      "  Processed 700,000,000 items\n",
      "  Processed 710,000,000 items\n",
      "  Processed 720,000,000 items\n",
      "  Processed 730,000,000 items\n",
      "  Processed 740,000,000 items\n",
      "  Processed 750,000,000 items\n",
      "  Processed 760,000,000 items\n",
      "  Processed 770,000,000 items\n",
      "  Processed 780,000,000 items\n",
      "  Processed 790,000,000 items\n",
      "  Processed 800,000,000 items\n",
      "  Processed 810,000,000 items\n",
      "  Processed 820,000,000 items\n",
      "  Processed 830,000,000 items\n",
      "  Processed 840,000,000 items\n",
      "  Processed 850,000,000 items\n",
      "  Processed 860,000,000 items\n",
      "  Processed 870,000,000 items\n",
      "  Processed 880,000,000 items\n",
      "  Processed 890,000,000 items\n",
      "  Processed 900,000,000 items\n",
      "  Processed 910,000,000 items\n",
      "  Processed 920,000,000 items\n",
      "  Processed 930,000,000 items\n",
      "  Processed 940,000,000 items\n",
      "  Processed 950,000,000 items\n",
      "  Processed 960,000,000 items\n",
      "  Processed 970,000,000 items\n",
      "  Processed 980,000,000 items\n",
      "  Processed 990,000,000 items\n",
      "  Processed 1,000,000,000 items\n",
      "  Processed 1,010,000,000 items\n",
      "  Processed 1,020,000,000 items\n",
      "  Processed 1,030,000,000 items\n",
      "  Processed 1,040,000,000 items\n",
      "  Processed 1,050,000,000 items\n",
      "  Processed 1,060,000,000 items\n",
      "  Processed 1,070,000,000 items\n",
      "  Processed 1,080,000,000 items\n",
      "  Processed 1,090,000,000 items\n",
      "  Processed 1,100,000,000 items\n",
      "  Processed 1,110,000,000 items\n",
      "  Processed 1,120,000,000 items\n",
      "  Processed 1,130,000,000 items\n",
      "  Processed 1,140,000,000 items\n",
      "  Processed 1,150,000,000 items\n",
      "  Processed 1,160,000,000 items\n",
      "  Processed 1,170,000,000 items\n",
      "  Processed 1,180,000,000 items\n",
      "  Processed 1,190,000,000 items\n",
      "  Processed 1,200,000,000 items\n",
      "  Processed 1,210,000,000 items\n",
      "  Processed 1,220,000,000 items\n",
      "  Processed 1,230,000,000 items\n",
      "  Processed 1,240,000,000 items\n",
      "  Processed 1,250,000,000 items\n",
      "  Processed 1,260,000,000 items\n",
      "  Processed 1,270,000,000 items\n",
      "  Processed 1,280,000,000 items\n",
      "  Processed 1,290,000,000 items\n",
      "  Processed 1,300,000,000 items\n",
      "  Processed 1,310,000,000 items\n",
      "  Processed 1,320,000,000 items\n",
      "  Processed 1,330,000,000 items\n",
      "  Processed 1,340,000,000 items\n",
      "  Processed 1,350,000,000 items\n",
      "  Processed 1,360,000,000 items\n",
      "  Processed 1,370,000,000 items\n",
      "  Processed 1,380,000,000 items\n",
      "  Processed 1,390,000,000 items\n",
      "  Processed 1,400,000,000 items\n",
      "  Processed 1,410,000,000 items\n",
      "  Processed 1,420,000,000 items\n",
      "  Processed 1,430,000,000 items\n",
      "  Processed 1,440,000,000 items\n",
      "  Processed 1,450,000,000 items\n",
      "  Processed 1,460,000,000 items\n",
      "  Processed 1,470,000,000 items\n",
      "  Processed 1,480,000,000 items\n",
      "  Processed 1,490,000,000 items\n",
      "  Processed 1,500,000,000 items\n",
      "  Processed 1,510,000,000 items\n",
      "  Processed 1,520,000,000 items\n",
      "  Processed 1,530,000,000 items\n",
      "  Processed 1,540,000,000 items\n",
      "  Processed 1,550,000,000 items\n",
      "  Processed 1,560,000,000 items\n",
      "  Processed 1,570,000,000 items\n",
      "  Processed 1,580,000,000 items\n",
      "  Processed 1,590,000,000 items\n",
      "  Processed 1,600,000,000 items\n",
      "  Processed 1,610,000,000 items\n",
      "  Processed 1,620,000,000 items\n",
      "  Processed 1,630,000,000 items\n",
      "  Processed 1,640,000,000 items\n",
      "  Processed 1,650,000,000 items\n",
      "  Processed 1,660,000,000 items\n",
      "  Processed 1,670,000,000 items\n",
      "  Processed 1,680,000,000 items\n",
      "  Processed 1,690,000,000 items\n",
      "  Processed 1,700,000,000 items\n",
      "  Processed 1,710,000,000 items\n",
      "  Processed 1,720,000,000 items\n",
      "  Processed 1,730,000,000 items\n",
      "  Processed 1,740,000,000 items\n",
      "  Processed 1,750,000,000 items\n",
      "  Processed 1,760,000,000 items\n",
      "  Processed 1,770,000,000 items\n",
      "  Processed 1,780,000,000 items\n",
      "  Processed 1,790,000,000 items\n",
      "  Processed 1,800,000,000 items\n",
      "  Processed 1,810,000,000 items\n",
      "  Processed 1,820,000,000 items\n",
      "  Processed 1,830,000,000 items\n",
      "  Processed 1,840,000,000 items\n",
      "  Processed 1,850,000,000 items\n",
      "  Processed 1,860,000,000 items\n",
      "  Processed 1,870,000,000 items\n",
      "  Processed 1,880,000,000 items\n",
      "  Processed 1,890,000,000 items\n",
      "  Processed 1,900,000,000 items\n",
      "  Processed 1,910,000,000 items\n",
      "  Processed 1,920,000,000 items\n",
      "  Processed 1,930,000,000 items\n",
      "  Processed 1,940,000,000 items\n",
      "  Processed 1,950,000,000 items\n",
      "  Processed 1,960,000,000 items\n",
      "  Processed 1,970,000,000 items\n",
      "  Processed 1,980,000,000 items\n",
      "  Processed 1,990,000,000 items\n",
      "  Processed 2,000,000,000 items\n",
      "  Processed 2,010,000,000 items\n",
      "  Processed 2,020,000,000 items\n",
      "  Processed 2,030,000,000 items\n",
      "  Processed 2,040,000,000 items\n",
      "  Processed 2,050,000,000 items\n",
      "  Processed 2,060,000,000 items\n",
      "  Processed 2,070,000,000 items\n",
      "  Processed 2,080,000,000 items\n",
      "  Processed 2,090,000,000 items\n",
      "  Processed 2,100,000,000 items\n",
      "  Processed 2,110,000,000 items\n",
      "  Processed 2,120,000,000 items\n",
      "  Processed 2,130,000,000 items\n",
      "  Processed 2,140,000,000 items\n",
      "  Processed 2,150,000,000 items\n",
      "  Processed 2,160,000,000 items\n",
      "  Processed 2,170,000,000 items\n",
      "  Processed 2,180,000,000 items\n",
      "  Processed 2,190,000,000 items\n",
      "  Processed 2,200,000,000 items\n",
      "  Processed 2,210,000,000 items\n",
      "  Processed 2,220,000,000 items\n",
      "  Processed 2,230,000,000 items\n",
      "  Processed 2,240,000,000 items\n",
      "  Processed 2,250,000,000 items\n",
      "  Processed 2,260,000,000 items\n",
      "  Processed 2,270,000,000 items\n",
      "  Processed 2,280,000,000 items\n",
      "  Processed 2,290,000,000 items\n",
      "  Processed 2,300,000,000 items\n",
      "  Processed 2,310,000,000 items\n",
      "  Processed 2,320,000,000 items\n",
      "  Processed 2,330,000,000 items\n",
      "  Processed 2,340,000,000 items\n",
      "  Processed 2,350,000,000 items\n",
      "  Processed 2,360,000,000 items\n",
      "  Processed 2,370,000,000 items\n",
      "  Processed 2,380,000,000 items\n",
      "  Processed 2,390,000,000 items\n",
      "  Processed 2,400,000,000 items\n",
      "  Processed 2,410,000,000 items\n",
      "  Processed 2,420,000,000 items\n",
      "  Processed 2,430,000,000 items\n",
      "  Processed 2,440,000,000 items\n",
      "  Processed 2,450,000,000 items\n",
      "  Processed 2,460,000,000 items\n",
      "  Processed 2,470,000,000 items\n",
      "  Processed 2,480,000,000 items\n",
      "  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "  RESERVOIR SAMPLING RESULTS\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Items processed:        2,480,117,477\n",
      "  Metadata entries:       32,377\n",
      "  Final sample size:      248,014\n",
      "  Execution time:         10727.4704 seconds\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  PERFORMANCE METRICS\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Processing rate:        231,193 items/second\n",
      "  Time per item:          4.33 microseconds\n",
      "  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "  Sampling complete: 248,014 samples collected, 0 unique prefixes\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must call sample_database_density() first",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m      6\u001B[39m pipeline_config = PipelineConfig(\n\u001B[32m      7\u001B[39m     src_db=src_db,\n\u001B[32m      8\u001B[39m     dst_db=dst_db,\n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m     progress_every_s=\u001B[32m60.0\u001B[39m,\n\u001B[32m     15\u001B[39m )\n\u001B[32m     17\u001B[39m filter_config = FilterConfig(\n\u001B[32m     18\u001B[39m     stop_set=stopwords,\n\u001B[32m     19\u001B[39m     lemma_gen=lemmatizer,\n\u001B[32m     20\u001B[39m     whitelist_path=wht_path\n\u001B[32m     21\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[43mbuild_processed_db\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpipeline_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilter_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/scratch/edk202/ngram-prep/src/ngram_filter/pipeline/orchestrator.py:467\u001B[39m, in \u001B[36mbuild_processed_db\u001B[39m\u001B[34m(pipeline_config, filter_config)\u001B[39m\n\u001B[32m    459\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    460\u001B[39m \u001B[33;03mMain entry point for the simplified ngram filtering pipeline.\u001B[39;00m\n\u001B[32m    461\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    464\u001B[39m \u001B[33;03m    filter_config: Configuration for ngram filtering\u001B[39;00m\n\u001B[32m    465\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    466\u001B[39m orchestrator = PipelineOrchestrator(pipeline_config, filter_config)\n\u001B[32m--> \u001B[39m\u001B[32m467\u001B[39m \u001B[43morchestrator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/scratch/edk202/ngram-prep/src/ngram_filter/pipeline/orchestrator.py:59\u001B[39m, in \u001B[36mPipelineOrchestrator.run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     56\u001B[39m \u001B[38;5;28mself\u001B[39m._prepare_whitelist()\n\u001B[32m     58\u001B[39m \u001B[38;5;66;03m# Execute pipeline phases\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_work_units\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[38;5;28mself\u001B[39m._process_work_units()\n\u001B[32m     61\u001B[39m \u001B[38;5;28mself\u001B[39m._merge_results()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/scratch/edk202/ngram-prep/src/ngram_filter/pipeline/orchestrator.py:212\u001B[39m, in \u001B[36mPipelineOrchestrator._create_work_units\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m force_restart:\n\u001B[32m    211\u001B[39m     work_tracker.clear_all_work_units()\n\u001B[32m--> \u001B[39m\u001B[32m212\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_new_work_units\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwork_tracker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_work_units\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    213\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    214\u001B[39m     progress = work_tracker.get_progress()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/scratch/edk202/ngram-prep/src/ngram_filter/pipeline/orchestrator.py:232\u001B[39m, in \u001B[36mPipelineOrchestrator._create_new_work_units\u001B[39m\u001B[34m(self, work_tracker, num_work_units)\u001B[39m\n\u001B[32m    230\u001B[39m sample_rate = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.pipeline_config, \u001B[33m'\u001B[39m\u001B[33mpartitioning_sample_rate\u001B[39m\u001B[33m'\u001B[39m, \u001B[32m0.001\u001B[39m)\n\u001B[32m    231\u001B[39m prefix_length = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.pipeline_config, \u001B[33m'\u001B[39m\u001B[33mprefix_length\u001B[39m\u001B[33m'\u001B[39m, \u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m232\u001B[39m work_units = \u001B[43mcreate_intelligent_work_units\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtemp_paths\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43msrc_db\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    234\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_work_units\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    235\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    236\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprefix_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprefix_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    237\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    239\u001B[39m work_tracker.add_work_units(work_units)\n\u001B[32m    240\u001B[39m progress = work_tracker.get_progress()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/scratch/edk202/ngram-prep/src/ngram_filter/partitioning/intelligent.py:217\u001B[39m, in \u001B[36mcreate_intelligent_work_units\u001B[39m\u001B[34m(src_db_path, num_units, sample_rate, prefix_length)\u001B[39m\n\u001B[32m    214\u001B[39m partitioner.sample_database_density(prefix_length=prefix_length)\n\u001B[32m    216\u001B[39m \u001B[38;5;66;03m# Create balanced work units\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m work_units = \u001B[43mpartitioner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_balanced_work_units\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_units\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[38;5;66;03m# Analyze the balance\u001B[39;00m\n\u001B[32m    220\u001B[39m partitioner.analyze_balance(work_units)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/scratch/edk202/ngram-prep/src/ngram_filter/partitioning/intelligent.py:91\u001B[39m, in \u001B[36mIntelligentPartitioner.create_balanced_work_units\u001B[39m\u001B[34m(self, num_units, target_records_per_unit)\u001B[39m\n\u001B[32m     80\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     81\u001B[39m \u001B[33;03mCreate work units with roughly equal estimated record counts.\u001B[39;00m\n\u001B[32m     82\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m     88\u001B[39m \u001B[33;03m    List of WorkUnit objects with balanced workloads\u001B[39;00m\n\u001B[32m     89\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.density_map:\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mMust call sample_database_density() first\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     93\u001B[39m total_estimated_records = \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mself\u001B[39m.density_map.values())\n\u001B[32m     94\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m target_records_per_unit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mValueError\u001B[39m: Must call sample_database_density() first"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T02:16:52.152719Z",
     "start_time": "2025-09-21T02:16:51.210491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utilities.check_db_props import check_db_properties\n",
    "\n",
    "src_db = Path(\"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\")\n",
    "\n",
    "check_db_properties(src_db)"
   ],
   "id": "7e76c8db03a83cf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available RocksDB Properties:\n",
      "==================================================\n",
      "âœ“ rocksdb.estimate-num-keys: 2480149854\n",
      "âœ“ rocksdb.estimate-table-readers-mem: 11020464\n",
      "âœ“ rocksdb.size-all-mem-tables: 2048\n",
      "âœ“ rocksdb.num-entries-active-mem-table: 0\n",
      "âœ“ rocksdb.estimate-live-data-size: 2433047738084\n",
      "âœ“ rocksdb.total-sst-files-size: 2433047738084\n",
      "âœ“ rocksdb.num-live-versions: 1\n",
      "âœ“ rocksdb.base-level: 3\n",
      "âœ“ rocksdb.estimate-pending-compaction-bytes: 0\n",
      "Available RocksDB Properties:\n",
      "==================================================\n",
      "âœ“ rocksdb.estimate-num-keys: 2480149854\n",
      "âœ“ rocksdb.estimate-table-readers-mem: 11020464\n",
      "âœ“ rocksdb.size-all-mem-tables: 2048\n",
      "âœ“ rocksdb.num-entries-active-mem-table: 0\n",
      "âœ“ rocksdb.estimate-live-data-size: 2433047738084\n",
      "âœ“ rocksdb.total-sst-files-size: 2433047738084\n",
      "âœ“ rocksdb.num-live-versions: 1\n",
      "âœ“ rocksdb.base-level: 3\n",
      "âœ“ rocksdb.estimate-pending-compaction-bytes: 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5d729171f5d4e6ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist_w2v (Singularity)",
   "language": "python",
   "name": "hist_w2v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
